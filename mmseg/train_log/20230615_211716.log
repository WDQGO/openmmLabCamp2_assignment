2023/06/15 21:17:21 - mmengine - INFO - 
------------------------------------------------------------
System environment:
    sys.platform: win32
    Python: 3.9.16 (main, Mar  8 2023, 10:39:24) [MSC v.1916 64 bit (AMD64)]
    CUDA available: True
    numpy_random_seed: 1765663822
    GPU 0: NVIDIA GeForce RTX 3080
    CUDA_HOME: C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.2
    NVCC: Cuda compilation tools, release 11.2, V11.2.152
    MSVC: 用于 x64 的 Microsoft (R) C/C++ 优化编译器 19.29.30146 版
    GCC: n/a
    PyTorch: 2.0.1+cu117
    PyTorch compiling details: PyTorch built with:
  - C++ Version: 199711
  - MSVC 193431937
  - Intel(R) Math Kernel Library Version 2020.0.2 Product Build 20200624 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 2019
  - LAPACK is enabled (usually provided by MKL)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_37,code=sm_37;-gencode;arch=compute_50,code=sm_50;-gencode;arch=compute_60,code=sm_60;-gencode;arch=compute_61,code=sm_61;-gencode;arch=compute_70,code=sm_70;-gencode;arch=compute_75,code=sm_75;-gencode;arch=compute_80,code=sm_80;-gencode;arch=compute_86,code=sm_86;-gencode;arch=compute_37,code=compute_37
  - CuDNN 8.5
  - Magma 2.5.4
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=C:/actions-runner/_work/pytorch/pytorch/builder/windows/tmp_bin/sccache-cl.exe, CXX_FLAGS=/DWIN32 /D_WINDOWS /GR /EHsc /w /bigobj /FS -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=OFF, TORCH_VERSION=2.0.1, USE_CUDA=ON, USE_CUDNN=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=OFF, USE_OPENMP=ON, USE_ROCM=OFF, 

    TorchVision: 0.15.2+cu117
    OpenCV: 4.7.0
    MMEngine: 0.7.3

Runtime environment:
    cudnn_benchmark: True
    mp_cfg: {'mp_start_method': 'fork', 'opencv_num_threads': 0}
    dist_cfg: {'backend': 'nccl'}
    seed: None
    Distributed launcher: none
    Distributed training: False
    GPU number: 1
------------------------------------------------------------

2023/06/15 21:17:21 - mmengine - INFO - Config:
default_scope = 'mmseg'
env_cfg = dict(
    cudnn_benchmark=True,
    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),
    dist_cfg=dict(backend='nccl'))
vis_backends = [dict(type='LocalVisBackend')]
visualizer = dict(
    type='SegLocalVisualizer',
    vis_backends=[dict(type='LocalVisBackend')],
    name='visualizer')
log_processor = dict(by_epoch=False)
log_level = 'INFO'
load_from = None
resume = False
tta_model = dict(type='SegTTAModel')
dataset_type = 'WatermelonDataset'
data_root = 'E:\\dataSetCollect\\mmseg\\Watermelon87_Semantic_Seg_Mask'
crop_size = (640, 640)
train_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='LoadAnnotations', reduce_zero_label=False),
    dict(
        type='RandomChoiceResize',
        scales=[
            320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960, 1024, 1088,
            1152, 1216, 1280
        ],
        resize_type='ResizeShortestEdge',
        max_size=2560),
    dict(type='RandomCrop', crop_size=(640, 640), cat_max_ratio=0.75),
    dict(type='RandomFlip', prob=0.5),
    dict(type='PhotoMetricDistortion'),
    dict(type='PackSegInputs')
]
test_pipeline = [
    dict(type='LoadImageFromFile'),
    dict(type='Resize', scale=(2560, 640), keep_ratio=True),
    dict(type='LoadAnnotations', reduce_zero_label=False),
    dict(type='PackSegInputs')
]
img_ratios = [0.5, 0.75, 1.0, 1.25, 1.5, 1.75]
tta_pipeline = [
    dict(type='LoadImageFromFile', backend_args=None),
    dict(
        type='TestTimeAug',
        transforms=[[{
            'type': 'Resize',
            'scale_factor': 0.5,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 0.75,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.0,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.25,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.5,
            'keep_ratio': True
        }, {
            'type': 'Resize',
            'scale_factor': 1.75,
            'keep_ratio': True
        }],
                    [{
                        'type': 'RandomFlip',
                        'prob': 0.0,
                        'direction': 'horizontal'
                    }, {
                        'type': 'RandomFlip',
                        'prob': 1.0,
                        'direction': 'horizontal'
                    }], [{
                        'type': 'LoadAnnotations'
                    }], [{
                        'type': 'PackSegInputs'
                    }]])
]
train_dataloader = dict(
    batch_size=2,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(type='InfiniteSampler', shuffle=True),
    dataset=dict(
        type='WatermelonDataset',
        data_root='E:\\dataSetCollect\\mmseg\\Watermelon87_Semantic_Seg_Mask',
        data_prefix=dict(
            img_path='img_dir/train', seg_map_path='ann_dir/train'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='LoadAnnotations', reduce_zero_label=False),
            dict(
                type='RandomChoiceResize',
                scales=[
                    320, 384, 448, 512, 576, 640, 704, 768, 832, 896, 960,
                    1024, 1088, 1152, 1216, 1280
                ],
                resize_type='ResizeShortestEdge',
                max_size=2560),
            dict(type='RandomCrop', crop_size=(640, 640), cat_max_ratio=0.75),
            dict(type='RandomFlip', prob=0.5),
            dict(type='PhotoMetricDistortion'),
            dict(type='PackSegInputs')
        ]))
val_dataloader = dict(
    batch_size=1,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='WatermelonDataset',
        data_root='E:\\dataSetCollect\\mmseg\\Watermelon87_Semantic_Seg_Mask',
        data_prefix=dict(img_path='img_dir/val', seg_map_path='ann_dir/val'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', scale=(2560, 640), keep_ratio=True),
            dict(type='LoadAnnotations', reduce_zero_label=False),
            dict(type='PackSegInputs')
        ]))
test_dataloader = dict(
    batch_size=1,
    num_workers=4,
    persistent_workers=True,
    sampler=dict(type='DefaultSampler', shuffle=False),
    dataset=dict(
        type='WatermelonDataset',
        data_root='E:\\dataSetCollect\\mmseg\\Watermelon87_Semantic_Seg_Mask',
        data_prefix=dict(img_path='img_dir/val', seg_map_path='ann_dir/val'),
        pipeline=[
            dict(type='LoadImageFromFile'),
            dict(type='Resize', scale=(2560, 640), keep_ratio=True),
            dict(type='LoadAnnotations', reduce_zero_label=False),
            dict(type='PackSegInputs')
        ]))
val_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])
test_evaluator = dict(type='IoUMetric', iou_metrics=['mIoU'])
pretrained = 'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/swin/swin_base_patch4_window12_384_20220317-55b0104a.pth'
data_preprocessor = dict(
    type='SegDataPreProcessor',
    mean=[123.675, 116.28, 103.53],
    std=[58.395, 57.12, 57.375],
    bgr_to_rgb=True,
    pad_val=0,
    seg_pad_val=255,
    size=(640, 640))
num_classes = 150
depths = [2, 2, 18, 2]
model = dict(
    type='EncoderDecoder',
    data_preprocessor=dict(
        type='SegDataPreProcessor',
        mean=[123.675, 116.28, 103.53],
        std=[58.395, 57.12, 57.375],
        bgr_to_rgb=True,
        pad_val=0,
        seg_pad_val=255,
        size=(640, 640)),
    backbone=dict(
        type='SwinTransformer',
        pretrain_img_size=384,
        embed_dims=128,
        depths=[2, 2, 18, 2],
        num_heads=[4, 8, 16, 32],
        window_size=12,
        mlp_ratio=4,
        qkv_bias=True,
        qk_scale=None,
        drop_rate=0.0,
        attn_drop_rate=0.0,
        drop_path_rate=0.3,
        patch_norm=True,
        out_indices=(0, 1, 2, 3),
        with_cp=False,
        frozen_stages=-1,
        init_cfg=dict(
            type='Pretrained',
            checkpoint=
            'https://download.openmmlab.com/mmsegmentation/v0.5/pretrain/swin/swin_base_patch4_window12_384_20220317-55b0104a.pth'
        )),
    decode_head=dict(
        type='Mask2FormerHead',
        in_channels=[128, 256, 512, 1024],
        strides=[4, 8, 16, 32],
        feat_channels=256,
        out_channels=256,
        num_classes=150,
        num_queries=100,
        num_transformer_feat_level=3,
        align_corners=False,
        pixel_decoder=dict(
            type='mmdet.MSDeformAttnPixelDecoder',
            num_outs=3,
            norm_cfg=dict(type='GN', num_groups=32),
            act_cfg=dict(type='ReLU'),
            encoder=dict(
                num_layers=6,
                layer_cfg=dict(
                    self_attn_cfg=dict(
                        embed_dims=256,
                        num_heads=8,
                        num_levels=3,
                        num_points=4,
                        im2col_step=64,
                        dropout=0.0,
                        batch_first=True,
                        norm_cfg=None,
                        init_cfg=None),
                    ffn_cfg=dict(
                        embed_dims=256,
                        feedforward_channels=1024,
                        num_fcs=2,
                        ffn_drop=0.0,
                        act_cfg=dict(type='ReLU', inplace=True))),
                init_cfg=None),
            positional_encoding=dict(num_feats=128, normalize=True),
            init_cfg=None),
        enforce_decoder_input_project=False,
        positional_encoding=dict(num_feats=128, normalize=True),
        transformer_decoder=dict(
            return_intermediate=True,
            num_layers=9,
            layer_cfg=dict(
                self_attn_cfg=dict(
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=True),
                cross_attn_cfg=dict(
                    embed_dims=256,
                    num_heads=8,
                    attn_drop=0.0,
                    proj_drop=0.0,
                    dropout_layer=None,
                    batch_first=True),
                ffn_cfg=dict(
                    embed_dims=256,
                    feedforward_channels=2048,
                    num_fcs=2,
                    act_cfg=dict(type='ReLU', inplace=True),
                    ffn_drop=0.0,
                    dropout_layer=None,
                    add_identity=True)),
            init_cfg=None),
        loss_cls=dict(
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=False,
            loss_weight=2.0,
            reduction='mean',
            class_weight=[
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0,
                1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1
            ]),
        loss_mask=dict(
            type='mmdet.CrossEntropyLoss',
            use_sigmoid=True,
            reduction='mean',
            loss_weight=5.0),
        loss_dice=dict(
            type='mmdet.DiceLoss',
            use_sigmoid=True,
            activate=True,
            reduction='mean',
            naive_dice=True,
            eps=1.0,
            loss_weight=5.0),
        train_cfg=dict(
            num_points=12544,
            oversample_ratio=3.0,
            importance_sample_ratio=0.75,
            assigner=dict(
                type='mmdet.HungarianAssigner',
                match_costs=[
                    dict(type='mmdet.ClassificationCost', weight=2.0),
                    dict(
                        type='mmdet.CrossEntropyLossCost',
                        weight=5.0,
                        use_sigmoid=True),
                    dict(
                        type='mmdet.DiceCost',
                        weight=5.0,
                        pred_act=True,
                        eps=1.0)
                ]),
            sampler=dict(type='mmdet.MaskPseudoSampler'))),
    train_cfg=dict(),
    test_cfg=dict(mode='whole'))
backbone_norm_multi = dict(lr_mult=0.1, decay_mult=0.0)
backbone_embed_multi = dict(lr_mult=0.1, decay_mult=0.0)
embed_multi = dict(lr_mult=1.0, decay_mult=0.0)
custom_keys = dict({
    'backbone':
    dict(lr_mult=0.1, decay_mult=1.0),
    'backbone.patch_embed.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'absolute_pos_embed':
    dict(lr_mult=0.1, decay_mult=0.0),
    'relative_position_bias_table':
    dict(lr_mult=0.1, decay_mult=0.0),
    'query_embed':
    dict(lr_mult=1.0, decay_mult=0.0),
    'query_feat':
    dict(lr_mult=1.0, decay_mult=0.0),
    'level_embed':
    dict(lr_mult=1.0, decay_mult=0.0),
    'backbone.stages.0.blocks.0.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.0.blocks.1.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.1.blocks.0.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.1.blocks.1.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.0.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.1.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.2.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.3.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.4.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.5.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.6.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.7.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.8.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.9.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.10.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.11.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.12.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.13.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.14.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.15.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.16.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.blocks.17.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.3.blocks.0.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.3.blocks.1.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.0.downsample.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.1.downsample.norm':
    dict(lr_mult=0.1, decay_mult=0.0),
    'backbone.stages.2.downsample.norm':
    dict(lr_mult=0.1, decay_mult=0.0)
})
optimizer = dict(
    type='AdamW', lr=0.0001, weight_decay=0.05, eps=1e-08, betas=(0.9, 0.999))
optim_wrapper = dict(
    type='OptimWrapper',
    optimizer=dict(
        type='AdamW',
        lr=0.0001,
        weight_decay=0.05,
        eps=1e-08,
        betas=(0.9, 0.999)),
    clip_grad=dict(max_norm=0.01, norm_type=2),
    paramwise_cfg=dict(
        custom_keys=dict({
            'backbone':
            dict(lr_mult=0.1, decay_mult=1.0),
            'backbone.patch_embed.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'absolute_pos_embed':
            dict(lr_mult=0.1, decay_mult=0.0),
            'relative_position_bias_table':
            dict(lr_mult=0.1, decay_mult=0.0),
            'query_embed':
            dict(lr_mult=1.0, decay_mult=0.0),
            'query_feat':
            dict(lr_mult=1.0, decay_mult=0.0),
            'level_embed':
            dict(lr_mult=1.0, decay_mult=0.0),
            'backbone.stages.0.blocks.0.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.0.blocks.1.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.1.blocks.0.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.1.blocks.1.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.0.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.1.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.2.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.3.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.4.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.5.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.6.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.7.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.8.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.9.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.10.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.11.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.12.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.13.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.14.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.15.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.16.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.blocks.17.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.3.blocks.0.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.3.blocks.1.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.0.downsample.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.1.downsample.norm':
            dict(lr_mult=0.1, decay_mult=0.0),
            'backbone.stages.2.downsample.norm':
            dict(lr_mult=0.1, decay_mult=0.0)
        }),
        norm_decay_mult=0.0))
param_scheduler = [
    dict(
        type='PolyLR',
        eta_min=0,
        power=0.9,
        begin=0,
        end=160000,
        by_epoch=False)
]
train_cfg = dict(type='IterBasedTrainLoop', max_iters=3000, val_interval=300)
val_cfg = dict(type='ValLoop')
test_cfg = dict(type='TestLoop')
default_hooks = dict(
    timer=dict(type='IterTimerHook'),
    logger=dict(type='LoggerHook', interval=30, log_metric_by_epoch=False),
    param_scheduler=dict(type='ParamSchedulerHook'),
    checkpoint=dict(
        type='CheckpointHook',
        by_epoch=False,
        interval=300,
        save_best='mIoU',
        max_keep_ckpts=3),
    sampler_seed=dict(type='DistSamplerSeedHook'),
    visualization=dict(type='SegVisualizationHook'))
auto_scale_lr = dict(enable=False, base_batch_size=16)
launcher = 'none'
work_dir = './work_dirs\\AwatermelonMask2Former'

2023/06/15 21:17:23 - mmengine - INFO - Distributed training is not used, all SyncBatchNorm (SyncBN) layers in the model will be automatically reverted to BatchNormXd layers if they are used.
2023/06/15 21:17:23 - mmengine - INFO - Hooks will be executed in the following order:
before_run:
(VERY_HIGH   ) RuntimeInfoHook                    
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
before_train:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_train_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) DistSamplerSeedHook                
 -------------------- 
before_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_train_iter:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SegVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train_epoch:
(NORMAL      ) IterTimerHook                      
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_val_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_val_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_val_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SegVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_val_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
(LOW         ) ParamSchedulerHook                 
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
after_train:
(VERY_LOW    ) CheckpointHook                     
 -------------------- 
before_test_epoch:
(NORMAL      ) IterTimerHook                      
 -------------------- 
before_test_iter:
(NORMAL      ) IterTimerHook                      
 -------------------- 
after_test_iter:
(NORMAL      ) IterTimerHook                      
(NORMAL      ) SegVisualizationHook               
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_test_epoch:
(VERY_HIGH   ) RuntimeInfoHook                    
(NORMAL      ) IterTimerHook                      
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
after_run:
(BELOW_NORMAL) LoggerHook                         
 -------------------- 
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.projection.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.patch_embed.norm.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm1.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.norm2.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.0.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm1.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.norm2.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.blocks.1.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.norm.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.0.downsample.reduction.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm1.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.norm2.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.0.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.weight:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm1.bias:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:30 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.blocks.1.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.norm.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.1.downsample.reduction.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.0.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.1.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.2.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.3.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.4.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.5.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.6.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.7.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.8.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.9.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.10.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.11.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.12.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.13.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.14.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.15.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.16.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.blocks.17.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.norm.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.2.downsample.reduction.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.0.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.qkv.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.attn.w_msa.proj.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.0.0.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.weight:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:weight_decay=0.05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.stages.3.blocks.1.ffn.layers.1.bias:decay_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm0.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm1.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm2.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr=1e-05
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:lr_mult=0.1
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- backbone.norm3.bias:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.0.gn.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.1.gn.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.input_convs.2.gn.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.0.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.1.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.2.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.3.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.4.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.encoder.layers.5.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.lateral_convs.0.gn.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.pixel_decoder.output_convs.0.gn.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.0.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.1.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.2.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.3.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.4.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.5.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.6.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.7.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.0.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.1.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.layers.8.norms.2.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.transformer_decoder.post_norm.bias:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr=0.0001
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:lr_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_embed.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr=0.0001
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:lr_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.query_feat.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr=0.0001
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:weight_decay=0.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:lr_mult=1.0
2023/06/15 21:17:31 - mmengine - INFO - paramwise_options -- decode_head.level_embed.weight:decay_mult=0.0
2023/06/15 21:17:31 - mmengine - WARNING - The prefix is not set in metric class IoUMetric.
Name of parameter - Initialization information

backbone.patch_embed.projection.weight - torch.Size([128, 3, 4, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.projection.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.patch_embed.norm.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.qkv.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.attn.w_msa.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.0.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.0.ffn.layers.1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 4]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.weight - torch.Size([384, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.qkv.bias - torch.Size([384]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.weight - torch.Size([128, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.attn.w_msa.proj.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.weight - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.norm2.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.weight - torch.Size([512, 128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.0.0.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.weight - torch.Size([128, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.blocks.1.ffn.layers.1.bias - torch.Size([128]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.norm.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.0.downsample.reduction.weight - torch.Size([256, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.qkv.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.attn.w_msa.proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.norm2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.0.ffn.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 8]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.qkv.bias - torch.Size([768]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.attn.w_msa.proj.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.weight - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.norm2.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.0.0.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.blocks.1.ffn.layers.1.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.norm.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.1.downsample.reduction.weight - torch.Size([512, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.0.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.1.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.2.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.3.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.4.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.5.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.6.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.7.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.8.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.9.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.10.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.11.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.12.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.13.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.14.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.15.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.16.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.relative_position_bias_table - torch.Size([529, 16]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.weight - torch.Size([1536, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.qkv.bias - torch.Size([1536]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.weight - torch.Size([512, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.attn.w_msa.proj.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.weight - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.norm2.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.weight - torch.Size([2048, 512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.0.0.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.weight - torch.Size([512, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.blocks.17.ffn.layers.1.bias - torch.Size([512]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.weight - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.norm.bias - torch.Size([2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.2.downsample.reduction.weight - torch.Size([1024, 2048]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.qkv.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.attn.w_msa.proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.norm2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.0.0.bias - torch.Size([4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.0.ffn.layers.1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.relative_position_bias_table - torch.Size([529, 32]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.weight - torch.Size([3072, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.qkv.bias - torch.Size([3072]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.weight - torch.Size([1024, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.attn.w_msa.proj.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.weight - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.norm2.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.weight - torch.Size([4096, 1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.0.0.bias - torch.Size([4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.weight - torch.Size([1024, 4096]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.stages.3.blocks.1.ffn.layers.1.bias - torch.Size([1024]): 
Initialized by user-defined `init_weights` in SwinTransformer  

backbone.norm0.weight - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm0.bias - torch.Size([128]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm2.weight - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm2.bias - torch.Size([512]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm3.weight - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

backbone.norm3.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.0.conv.weight - torch.Size([256, 1024, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.0.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.1.conv.weight - torch.Size([256, 512, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.1.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.1.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.1.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.2.conv.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.input_convs.2.conv.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.2.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.input_convs.2.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.weight - torch.Size([192, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.sampling_offsets.bias - torch.Size([192]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.weight - torch.Size([96, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.attention_weights.bias - torch.Size([96]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.value_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.self_attn.output_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.weight - torch.Size([1024, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.0.0.bias - torch.Size([1024]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.weight - torch.Size([256, 1024]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.encoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.encoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.level_encoding.weight - torch.Size([3, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.conv.weight - torch.Size([256, 128, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.lateral_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.lateral_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.output_convs.0.conv.weight - torch.Size([256, 256, 3, 3]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.output_convs.0.gn.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.output_convs.0.gn.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.pixel_decoder.mask_feature.weight - torch.Size([256, 256, 1, 1]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.pixel_decoder.mask_feature.bias - torch.Size([256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.0.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.0.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.1.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.1.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.2.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.2.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.3.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.3.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.4.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.4.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.5.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.5.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.6.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.6.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.7.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.7.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.self_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.self_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_weight - torch.Size([768, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.cross_attn.attn.in_proj_bias - torch.Size([768]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.weight - torch.Size([256, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.cross_attn.attn.out_proj.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.weight - torch.Size([2048, 256]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.ffn.layers.0.0.bias - torch.Size([2048]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.ffn.layers.1.weight - torch.Size([256, 2048]): 
Initialized by user-defined `init_weights` in Mask2FormerHead  

decode_head.transformer_decoder.layers.8.ffn.layers.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.0.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.1.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.1.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.2.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.layers.8.norms.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.post_norm.weight - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.transformer_decoder.post_norm.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.query_embed.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.query_feat.weight - torch.Size([100, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.level_embed.weight - torch.Size([3, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.cls_embed.weight - torch.Size([151, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.cls_embed.bias - torch.Size([151]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.0.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.0.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.2.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.2.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.4.weight - torch.Size([256, 256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  

decode_head.mask_embed.4.bias - torch.Size([256]): 
The value is the same before and after calling `init_weights` of EncoderDecoder  
2023/06/15 21:17:34 - mmengine - WARNING - "FileClient" will be deprecated in future. Please use io functions in https://mmengine.readthedocs.io/en/latest/api/fileio.html#file-io
2023/06/15 21:17:34 - mmengine - WARNING - "HardDiskBackend" is the alias of "LocalBackend" and the former will be deprecated in future.
2023/06/15 21:17:34 - mmengine - INFO - Checkpoints will be saved to D:\ACpycharm\myProjeycs\mmsegmentation-main\tools\work_dirs\AwatermelonMask2Former.
2023/06/15 21:17:48 - mmengine - INFO - Exp name: AwatermelonMask2Former_20230615_211716
2023/06/15 21:17:48 - mmengine - INFO - Iter(train) [  23/3000]  lr: 9.9988e-06  eta: 0:30:24  time: 0.4660  data_time: 0.0043  memory: 9583  grad_norm: 228.1765  loss: 106.0225  decode.loss_cls: 3.5934  decode.loss_mask: 2.7026  decode.loss_dice: 4.1449  decode.d0.loss_cls: 10.1873  decode.d0.loss_mask: 1.8212  decode.d0.loss_dice: 3.6017  decode.d1.loss_cls: 3.9817  decode.d1.loss_mask: 2.0355  decode.d1.loss_dice: 3.7862  decode.d2.loss_cls: 3.5338  decode.d2.loss_mask: 2.0876  decode.d2.loss_dice: 3.7648  decode.d3.loss_cls: 3.5956  decode.d3.loss_mask: 2.1397  decode.d3.loss_dice: 3.9028  decode.d4.loss_cls: 3.5926  decode.d4.loss_mask: 2.2729  decode.d4.loss_dice: 4.0048  decode.d5.loss_cls: 3.5681  decode.d5.loss_mask: 2.4859  decode.d5.loss_dice: 4.0668  decode.d6.loss_cls: 3.6051  decode.d6.loss_mask: 2.5627  decode.d6.loss_dice: 4.1007  decode.d7.loss_cls: 3.6114  decode.d7.loss_mask: 2.6571  decode.d7.loss_dice: 4.1085  decode.d8.loss_cls: 3.6136  decode.d8.loss_mask: 2.7501  decode.d8.loss_dice: 4.1433
2023/06/15 21:17:51 - mmengine - INFO - Iter(train) [  30/3000]  lr: 9.9984e-06  eta: 0:28:35  time: 0.4620  data_time: 0.0046  memory: 8318  grad_norm: 161.8407  loss: 99.9091  decode.loss_cls: 3.2248  decode.loss_mask: 2.7161  decode.loss_dice: 4.0384  decode.d0.loss_cls: 10.1983  decode.d0.loss_mask: 1.8039  decode.d0.loss_dice: 3.4336  decode.d1.loss_cls: 3.4851  decode.d1.loss_mask: 1.8999  decode.d1.loss_dice: 3.5487  decode.d2.loss_cls: 3.0081  decode.d2.loss_mask: 1.9286  decode.d2.loss_dice: 3.5357  decode.d3.loss_cls: 3.0544  decode.d3.loss_mask: 2.0982  decode.d3.loss_dice: 3.6840  decode.d4.loss_cls: 3.1626  decode.d4.loss_mask: 2.1997  decode.d4.loss_dice: 3.8478  decode.d5.loss_cls: 3.1927  decode.d5.loss_mask: 2.4081  decode.d5.loss_dice: 3.8163  decode.d6.loss_cls: 3.2413  decode.d6.loss_mask: 2.5665  decode.d6.loss_dice: 3.9183  decode.d7.loss_cls: 3.2480  decode.d7.loss_mask: 2.6505  decode.d7.loss_dice: 3.9766  decode.d8.loss_cls: 3.2645  decode.d8.loss_mask: 2.7328  decode.d8.loss_dice: 4.0255
2023/06/15 21:18:05 - mmengine - INFO - Iter(train) [  60/3000]  lr: 9.9967e-06  eta: 0:25:29  time: 0.4645  data_time: 0.0042  memory: 8329  grad_norm: 303.5263  loss: 73.9859  decode.loss_cls: 2.8052  decode.loss_mask: 1.9528  decode.loss_dice: 3.1527  decode.d0.loss_cls: 10.0401  decode.d0.loss_mask: 1.4005  decode.d0.loss_dice: 2.7292  decode.d1.loss_cls: 2.2995  decode.d1.loss_mask: 1.4162  decode.d1.loss_dice: 2.5570  decode.d2.loss_cls: 1.9196  decode.d2.loss_mask: 1.4177  decode.d2.loss_dice: 2.5619  decode.d3.loss_cls: 1.9299  decode.d3.loss_mask: 1.4355  decode.d3.loss_dice: 2.5252  decode.d4.loss_cls: 2.0170  decode.d4.loss_mask: 1.4576  decode.d4.loss_dice: 2.5799  decode.d5.loss_cls: 2.3312  decode.d5.loss_mask: 1.5333  decode.d5.loss_dice: 2.7119  decode.d6.loss_cls: 2.4049  decode.d6.loss_mask: 1.5796  decode.d6.loss_dice: 2.6946  decode.d7.loss_cls: 2.5816  decode.d7.loss_mask: 1.7047  decode.d7.loss_dice: 2.8506  decode.d8.loss_cls: 2.7639  decode.d8.loss_mask: 1.7490  decode.d8.loss_dice: 2.8830
2023/06/15 21:18:19 - mmengine - INFO - Iter(train) [  90/3000]  lr: 9.9950e-06  eta: 0:24:18  time: 0.4607  data_time: 0.0043  memory: 8329  grad_norm: 353.4790  loss: 56.1863  decode.loss_cls: 1.7105  decode.loss_mask: 1.3829  decode.loss_dice: 2.1482  decode.d0.loss_cls: 9.8941  decode.d0.loss_mask: 1.2495  decode.d0.loss_dice: 2.1890  decode.d1.loss_cls: 1.5348  decode.d1.loss_mask: 1.2541  decode.d1.loss_dice: 2.0247  decode.d2.loss_cls: 1.0652  decode.d2.loss_mask: 1.3321  decode.d2.loss_dice: 2.0043  decode.d3.loss_cls: 1.0466  decode.d3.loss_mask: 1.3570  decode.d3.loss_dice: 2.0296  decode.d4.loss_cls: 1.0346  decode.d4.loss_mask: 1.3688  decode.d4.loss_dice: 2.1057  decode.d5.loss_cls: 1.1303  decode.d5.loss_mask: 1.4490  decode.d5.loss_dice: 2.0936  decode.d6.loss_cls: 1.2477  decode.d6.loss_mask: 1.4311  decode.d6.loss_dice: 2.1645  decode.d7.loss_cls: 1.3383  decode.d7.loss_mask: 1.4333  decode.d7.loss_dice: 2.1700  decode.d8.loss_cls: 1.4734  decode.d8.loss_mask: 1.4327  decode.d8.loss_dice: 2.0905
2023/06/15 21:18:32 - mmengine - INFO - Iter(train) [ 120/3000]  lr: 9.9933e-06  eta: 0:23:30  time: 0.4549  data_time: 0.0043  memory: 8329  grad_norm: 459.0267  loss: 45.0257  decode.loss_cls: 0.9372  decode.loss_mask: 1.2783  decode.loss_dice: 1.7355  decode.d0.loss_cls: 9.7504  decode.d0.loss_mask: 1.0757  decode.d0.loss_dice: 1.7007  decode.d1.loss_cls: 0.8399  decode.d1.loss_mask: 1.2034  decode.d1.loss_dice: 1.6087  decode.d2.loss_cls: 0.5695  decode.d2.loss_mask: 1.2185  decode.d2.loss_dice: 1.5986  decode.d3.loss_cls: 0.5218  decode.d3.loss_mask: 1.1795  decode.d3.loss_dice: 1.6456  decode.d4.loss_cls: 0.5169  decode.d4.loss_mask: 1.2812  decode.d4.loss_dice: 1.6936  decode.d5.loss_cls: 0.5773  decode.d5.loss_mask: 1.2202  decode.d5.loss_dice: 1.7046  decode.d6.loss_cls: 0.5991  decode.d6.loss_mask: 1.2811  decode.d6.loss_dice: 1.7434  decode.d7.loss_cls: 0.6336  decode.d7.loss_mask: 1.2917  decode.d7.loss_dice: 1.7749  decode.d8.loss_cls: 0.7763  decode.d8.loss_mask: 1.2852  decode.d8.loss_dice: 1.7829
2023/06/15 21:18:46 - mmengine - INFO - Iter(train) [ 150/3000]  lr: 9.9916e-06  eta: 0:22:58  time: 0.4577  data_time: 0.0040  memory: 8329  grad_norm: 463.7897  loss: 39.7413  decode.loss_cls: 0.5855  decode.loss_mask: 1.0974  decode.loss_dice: 1.6158  decode.d0.loss_cls: 9.5764  decode.d0.loss_mask: 0.9419  decode.d0.loss_dice: 1.5824  decode.d1.loss_cls: 0.6054  decode.d1.loss_mask: 0.9897  decode.d1.loss_dice: 1.5237  decode.d2.loss_cls: 0.4304  decode.d2.loss_mask: 0.9882  decode.d2.loss_dice: 1.4896  decode.d3.loss_cls: 0.3861  decode.d3.loss_mask: 1.0388  decode.d3.loss_dice: 1.5072  decode.d4.loss_cls: 0.3745  decode.d4.loss_mask: 1.0749  decode.d4.loss_dice: 1.5368  decode.d5.loss_cls: 0.4312  decode.d5.loss_mask: 1.0740  decode.d5.loss_dice: 1.5509  decode.d6.loss_cls: 0.4545  decode.d6.loss_mask: 1.0595  decode.d6.loss_dice: 1.5209  decode.d7.loss_cls: 0.4830  decode.d7.loss_mask: 1.0579  decode.d7.loss_dice: 1.5688  decode.d8.loss_cls: 0.5293  decode.d8.loss_mask: 1.0826  decode.d8.loss_dice: 1.5841
2023/06/15 21:19:00 - mmengine - INFO - Iter(train) [ 180/3000]  lr: 9.9899e-06  eta: 0:22:32  time: 0.4563  data_time: 0.0041  memory: 8329  grad_norm: 467.1271  loss: 34.6145  decode.loss_cls: 0.2506  decode.loss_mask: 0.8560  decode.loss_dice: 1.5260  decode.d0.loss_cls: 9.3264  decode.d0.loss_mask: 0.7860  decode.d0.loss_dice: 1.5708  decode.d1.loss_cls: 0.3463  decode.d1.loss_mask: 0.8779  decode.d1.loss_dice: 1.4816  decode.d2.loss_cls: 0.2446  decode.d2.loss_mask: 0.8432  decode.d2.loss_dice: 1.4371  decode.d3.loss_cls: 0.2043  decode.d3.loss_mask: 0.8205  decode.d3.loss_dice: 1.4427  decode.d4.loss_cls: 0.2041  decode.d4.loss_mask: 0.8128  decode.d4.loss_dice: 1.4300  decode.d5.loss_cls: 0.2078  decode.d5.loss_mask: 0.8337  decode.d5.loss_dice: 1.4580  decode.d6.loss_cls: 0.2565  decode.d6.loss_mask: 0.8370  decode.d6.loss_dice: 1.4364  decode.d7.loss_cls: 0.2270  decode.d7.loss_mask: 0.8433  decode.d7.loss_dice: 1.4775  decode.d8.loss_cls: 0.2437  decode.d8.loss_mask: 0.8482  decode.d8.loss_dice: 1.4846
2023/06/15 21:19:14 - mmengine - INFO - Iter(train) [ 210/3000]  lr: 9.9882e-06  eta: 0:22:09  time: 0.4556  data_time: 0.0041  memory: 8329  grad_norm: 381.4502  loss: 34.5735  decode.loss_cls: 0.4317  decode.loss_mask: 0.8884  decode.loss_dice: 1.3513  decode.d0.loss_cls: 9.1879  decode.d0.loss_mask: 0.7689  decode.d0.loss_dice: 1.4275  decode.d1.loss_cls: 0.3863  decode.d1.loss_mask: 0.8126  decode.d1.loss_dice: 1.2984  decode.d2.loss_cls: 0.3473  decode.d2.loss_mask: 0.8040  decode.d2.loss_dice: 1.2900  decode.d3.loss_cls: 0.3348  decode.d3.loss_mask: 0.8813  decode.d3.loss_dice: 1.3737  decode.d4.loss_cls: 0.3177  decode.d4.loss_mask: 0.8799  decode.d4.loss_dice: 1.3773  decode.d5.loss_cls: 0.3513  decode.d5.loss_mask: 0.8647  decode.d5.loss_dice: 1.3670  decode.d6.loss_cls: 0.3488  decode.d6.loss_mask: 0.8587  decode.d6.loss_dice: 1.3867  decode.d7.loss_cls: 0.3628  decode.d7.loss_mask: 0.8640  decode.d7.loss_dice: 1.3807  decode.d8.loss_cls: 0.3859  decode.d8.loss_mask: 0.8729  decode.d8.loss_dice: 1.3709
2023/06/15 21:19:27 - mmengine - INFO - Iter(train) [ 240/3000]  lr: 9.9866e-06  eta: 0:21:48  time: 0.4557  data_time: 0.0042  memory: 8329  grad_norm: 470.3400  loss: 31.0310  decode.loss_cls: 0.1615  decode.loss_mask: 0.7948  decode.loss_dice: 1.2980  decode.d0.loss_cls: 8.9513  decode.d0.loss_mask: 0.7265  decode.d0.loss_dice: 1.3514  decode.d1.loss_cls: 0.1869  decode.d1.loss_mask: 0.7933  decode.d1.loss_dice: 1.2477  decode.d2.loss_cls: 0.1396  decode.d2.loss_mask: 0.8064  decode.d2.loss_dice: 1.2587  decode.d3.loss_cls: 0.1286  decode.d3.loss_mask: 0.7982  decode.d3.loss_dice: 1.2760  decode.d4.loss_cls: 0.1168  decode.d4.loss_mask: 0.8121  decode.d4.loss_dice: 1.2819  decode.d5.loss_cls: 0.1425  decode.d5.loss_mask: 0.7814  decode.d5.loss_dice: 1.2638  decode.d6.loss_cls: 0.1520  decode.d6.loss_mask: 0.7942  decode.d6.loss_dice: 1.2620  decode.d7.loss_cls: 0.1375  decode.d7.loss_mask: 0.8152  decode.d7.loss_dice: 1.3073  decode.d8.loss_cls: 0.1362  decode.d8.loss_mask: 0.8122  decode.d8.loss_dice: 1.2970
2023/06/15 21:19:41 - mmengine - INFO - Iter(train) [ 270/3000]  lr: 9.9849e-06  eta: 0:21:29  time: 0.4564  data_time: 0.0041  memory: 8329  grad_norm: 557.1822  loss: 30.0556  decode.loss_cls: 0.1187  decode.loss_mask: 0.7030  decode.loss_dice: 1.3688  decode.d0.loss_cls: 8.7177  decode.d0.loss_mask: 0.5980  decode.d0.loss_dice: 1.3684  decode.d1.loss_cls: 0.1813  decode.d1.loss_mask: 0.7018  decode.d1.loss_dice: 1.3035  decode.d2.loss_cls: 0.1541  decode.d2.loss_mask: 0.6906  decode.d2.loss_dice: 1.2925  decode.d3.loss_cls: 0.1238  decode.d3.loss_mask: 0.6966  decode.d3.loss_dice: 1.3124  decode.d4.loss_cls: 0.0986  decode.d4.loss_mask: 0.7063  decode.d4.loss_dice: 1.3342  decode.d5.loss_cls: 0.1010  decode.d5.loss_mask: 0.7049  decode.d5.loss_dice: 1.3484  decode.d6.loss_cls: 0.1068  decode.d6.loss_mask: 0.7000  decode.d6.loss_dice: 1.3355  decode.d7.loss_cls: 0.1103  decode.d7.loss_mask: 0.6917  decode.d7.loss_dice: 1.3581  decode.d8.loss_cls: 0.1103  decode.d8.loss_mask: 0.6849  decode.d8.loss_dice: 1.3334
2023/06/15 21:19:55 - mmengine - INFO - Iter(train) [ 300/3000]  lr: 9.9832e-06  eta: 0:21:10  time: 0.4566  data_time: 0.0042  memory: 8329  grad_norm: 591.7738  loss: 29.5818  decode.loss_cls: 0.1565  decode.loss_mask: 0.7602  decode.loss_dice: 1.2386  decode.d0.loss_cls: 8.4507  decode.d0.loss_mask: 0.6387  decode.d0.loss_dice: 1.2315  decode.d1.loss_cls: 0.1967  decode.d1.loss_mask: 0.7030  decode.d1.loss_dice: 1.2027  decode.d2.loss_cls: 0.1600  decode.d2.loss_mask: 0.7361  decode.d2.loss_dice: 1.2192  decode.d3.loss_cls: 0.1673  decode.d3.loss_mask: 0.7393  decode.d3.loss_dice: 1.2246  decode.d4.loss_cls: 0.1277  decode.d4.loss_mask: 0.7549  decode.d4.loss_dice: 1.2541  decode.d5.loss_cls: 0.1573  decode.d5.loss_mask: 0.7514  decode.d5.loss_dice: 1.2336  decode.d6.loss_cls: 0.1634  decode.d6.loss_mask: 0.7624  decode.d6.loss_dice: 1.2495  decode.d7.loss_cls: 0.1590  decode.d7.loss_mask: 0.7488  decode.d7.loss_dice: 1.2397  decode.d8.loss_cls: 0.1571  decode.d8.loss_mask: 0.7572  decode.d8.loss_dice: 1.2405
2023/06/15 21:19:55 - mmengine - INFO - Saving checkpoint at 300 iterations
2023/06/15 21:20:08 - mmengine - INFO - per class results:
2023/06/15 21:20:08 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 96.79 | 97.91 |
|    watermelon_green   | 93.39 | 98.46 |
|    watermelon_white   |  86.0 | 95.58 |
| watermelon_seed_black | 76.43 |  80.5 |
| watermelon_seed_white | 62.69 | 88.67 |
|       Unlabeled       | 16.76 | 17.06 |
+-----------------------+-------+-------+
2023/06/15 21:20:08 - mmengine - INFO - Iter(val) [11/11]    aAcc: 96.5100  mIoU: 72.0100  mAcc: 79.7000  data_time: 0.2440  time: 0.5233
2023/06/15 21:20:10 - mmengine - INFO - The best checkpoint with 72.0100 mIoU at 300 iter is saved to best_mIoU_iter_300.pth.
2023/06/15 21:20:24 - mmengine - INFO - Iter(train) [ 330/3000]  lr: 9.9815e-06  eta: 0:21:14  time: 0.4556  data_time: 0.0045  memory: 8331  grad_norm: 406.5348  loss: 26.3243  decode.loss_cls: 0.0898  decode.loss_mask: 0.7475  decode.loss_dice: 1.0413  decode.d0.loss_cls: 8.1432  decode.d0.loss_mask: 0.6268  decode.d0.loss_dice: 1.0650  decode.d1.loss_cls: 0.1075  decode.d1.loss_mask: 0.6913  decode.d1.loss_dice: 1.0356  decode.d2.loss_cls: 0.0853  decode.d2.loss_mask: 0.6952  decode.d2.loss_dice: 1.0550  decode.d3.loss_cls: 0.0832  decode.d3.loss_mask: 0.7012  decode.d3.loss_dice: 1.0576  decode.d4.loss_cls: 0.0734  decode.d4.loss_mask: 0.6966  decode.d4.loss_dice: 1.0223  decode.d5.loss_cls: 0.0771  decode.d5.loss_mask: 0.7241  decode.d5.loss_dice: 1.0272  decode.d6.loss_cls: 0.0849  decode.d6.loss_mask: 0.7251  decode.d6.loss_dice: 1.0090  decode.d7.loss_cls: 0.0832  decode.d7.loss_mask: 0.7084  decode.d7.loss_dice: 1.0231  decode.d8.loss_cls: 0.0912  decode.d8.loss_mask: 0.7306  decode.d8.loss_dice: 1.0227
2023/06/15 21:20:38 - mmengine - INFO - Iter(train) [ 360/3000]  lr: 9.9798e-06  eta: 0:20:55  time: 0.4557  data_time: 0.0043  memory: 8331  grad_norm: 199.3217  loss: 22.3289  decode.loss_cls: 0.0769  decode.loss_mask: 0.4854  decode.loss_dice: 0.8811  decode.d0.loss_cls: 7.9037  decode.d0.loss_mask: 0.4645  decode.d0.loss_dice: 0.8921  decode.d1.loss_cls: 0.0926  decode.d1.loss_mask: 0.4865  decode.d1.loss_dice: 0.8621  decode.d2.loss_cls: 0.0759  decode.d2.loss_mask: 0.4979  decode.d2.loss_dice: 0.8809  decode.d3.loss_cls: 0.0744  decode.d3.loss_mask: 0.5002  decode.d3.loss_dice: 0.8869  decode.d4.loss_cls: 0.0648  decode.d4.loss_mask: 0.5035  decode.d4.loss_dice: 0.8695  decode.d5.loss_cls: 0.0670  decode.d5.loss_mask: 0.5003  decode.d5.loss_dice: 0.8775  decode.d6.loss_cls: 0.0712  decode.d6.loss_mask: 0.4962  decode.d6.loss_dice: 0.8934  decode.d7.loss_cls: 0.0735  decode.d7.loss_mask: 0.4905  decode.d7.loss_dice: 0.8969  decode.d8.loss_cls: 0.0719  decode.d8.loss_mask: 0.4935  decode.d8.loss_dice: 0.8979
2023/06/15 21:20:51 - mmengine - INFO - Iter(train) [ 390/3000]  lr: 9.9781e-06  eta: 0:20:37  time: 0.4567  data_time: 0.0043  memory: 8331  grad_norm: 254.2658  loss: 23.0596  decode.loss_cls: 0.0739  decode.loss_mask: 0.5693  decode.loss_dice: 0.9355  decode.d0.loss_cls: 7.5977  decode.d0.loss_mask: 0.5051  decode.d0.loss_dice: 0.9696  decode.d1.loss_cls: 0.1102  decode.d1.loss_mask: 0.5431  decode.d1.loss_dice: 0.9126  decode.d2.loss_cls: 0.0879  decode.d2.loss_mask: 0.5510  decode.d2.loss_dice: 0.9198  decode.d3.loss_cls: 0.0790  decode.d3.loss_mask: 0.5590  decode.d3.loss_dice: 0.8957  decode.d4.loss_cls: 0.0748  decode.d4.loss_mask: 0.5587  decode.d4.loss_dice: 0.9078  decode.d5.loss_cls: 0.0712  decode.d5.loss_mask: 0.5747  decode.d5.loss_dice: 0.9160  decode.d6.loss_cls: 0.0735  decode.d6.loss_mask: 0.5673  decode.d6.loss_dice: 0.9001  decode.d7.loss_cls: 0.0710  decode.d7.loss_mask: 0.5760  decode.d7.loss_dice: 0.9074  decode.d8.loss_cls: 0.0683  decode.d8.loss_mask: 0.5803  decode.d8.loss_dice: 0.9033
2023/06/15 21:21:05 - mmengine - INFO - Iter(train) [ 420/3000]  lr: 9.9764e-06  eta: 0:20:19  time: 0.4588  data_time: 0.0046  memory: 8331  grad_norm: 202.9456  loss: 21.8423  decode.loss_cls: 0.0536  decode.loss_mask: 0.5082  decode.loss_dice: 0.9079  decode.d0.loss_cls: 7.3183  decode.d0.loss_mask: 0.4666  decode.d0.loss_dice: 0.9130  decode.d1.loss_cls: 0.0764  decode.d1.loss_mask: 0.4979  decode.d1.loss_dice: 0.8972  decode.d2.loss_cls: 0.0597  decode.d2.loss_mask: 0.5020  decode.d2.loss_dice: 0.8854  decode.d3.loss_cls: 0.0600  decode.d3.loss_mask: 0.5088  decode.d3.loss_dice: 0.8992  decode.d4.loss_cls: 0.0561  decode.d4.loss_mask: 0.5042  decode.d4.loss_dice: 0.9127  decode.d5.loss_cls: 0.0559  decode.d5.loss_mask: 0.5038  decode.d5.loss_dice: 0.9069  decode.d6.loss_cls: 0.0549  decode.d6.loss_mask: 0.4973  decode.d6.loss_dice: 0.9019  decode.d7.loss_cls: 0.0545  decode.d7.loss_mask: 0.5030  decode.d7.loss_dice: 0.8868  decode.d8.loss_cls: 0.0519  decode.d8.loss_mask: 0.5089  decode.d8.loss_dice: 0.8894
2023/06/15 21:21:19 - mmengine - INFO - Iter(train) [ 450/3000]  lr: 9.9747e-06  eta: 0:20:03  time: 0.4587  data_time: 0.0042  memory: 8331  grad_norm: 495.7984  loss: 23.2652  decode.loss_cls: 0.0721  decode.loss_mask: 0.5838  decode.loss_dice: 0.9602  decode.d0.loss_cls: 7.0329  decode.d0.loss_mask: 0.5786  decode.d0.loss_dice: 0.9975  decode.d1.loss_cls: 0.0891  decode.d1.loss_mask: 0.5897  decode.d1.loss_dice: 0.9505  decode.d2.loss_cls: 0.0703  decode.d2.loss_mask: 0.5801  decode.d2.loss_dice: 0.9680  decode.d3.loss_cls: 0.0688  decode.d3.loss_mask: 0.5932  decode.d3.loss_dice: 0.9653  decode.d4.loss_cls: 0.0643  decode.d4.loss_mask: 0.5807  decode.d4.loss_dice: 0.9749  decode.d5.loss_cls: 0.0648  decode.d5.loss_mask: 0.5955  decode.d5.loss_dice: 0.9929  decode.d6.loss_cls: 0.0676  decode.d6.loss_mask: 0.5951  decode.d6.loss_dice: 0.9726  decode.d7.loss_cls: 0.0740  decode.d7.loss_mask: 0.5897  decode.d7.loss_dice: 0.9733  decode.d8.loss_cls: 0.0724  decode.d8.loss_mask: 0.5858  decode.d8.loss_dice: 0.9615
2023/06/15 21:21:33 - mmengine - INFO - Iter(train) [ 480/3000]  lr: 9.9731e-06  eta: 0:19:46  time: 0.4595  data_time: 0.0050  memory: 8331  grad_norm: 153.2038  loss: 19.5157  decode.loss_cls: 0.0706  decode.loss_mask: 0.4709  decode.loss_dice: 0.7564  decode.d0.loss_cls: 6.6745  decode.d0.loss_mask: 0.4392  decode.d0.loss_dice: 0.7760  decode.d1.loss_cls: 0.0779  decode.d1.loss_mask: 0.4642  decode.d1.loss_dice: 0.7311  decode.d2.loss_cls: 0.0642  decode.d2.loss_mask: 0.4696  decode.d2.loss_dice: 0.7330  decode.d3.loss_cls: 0.0628  decode.d3.loss_mask: 0.4798  decode.d3.loss_dice: 0.7428  decode.d4.loss_cls: 0.0661  decode.d4.loss_mask: 0.4680  decode.d4.loss_dice: 0.7577  decode.d5.loss_cls: 0.0711  decode.d5.loss_mask: 0.4718  decode.d5.loss_dice: 0.7660  decode.d6.loss_cls: 0.0653  decode.d6.loss_mask: 0.4768  decode.d6.loss_dice: 0.7443  decode.d7.loss_cls: 0.0633  decode.d7.loss_mask: 0.4702  decode.d7.loss_dice: 0.7681  decode.d8.loss_cls: 0.0663  decode.d8.loss_mask: 0.4646  decode.d8.loss_dice: 0.7829
2023/06/15 21:21:46 - mmengine - INFO - Iter(train) [ 510/3000]  lr: 9.9714e-06  eta: 0:19:30  time: 0.4548  data_time: 0.0043  memory: 8320  grad_norm: 346.5216  loss: 20.4560  decode.loss_cls: 0.0523  decode.loss_mask: 0.4964  decode.loss_dice: 0.8638  decode.d0.loss_cls: 6.3871  decode.d0.loss_mask: 0.4731  decode.d0.loss_dice: 0.8425  decode.d1.loss_cls: 0.1128  decode.d1.loss_mask: 0.4933  decode.d1.loss_dice: 0.8587  decode.d2.loss_cls: 0.0569  decode.d2.loss_mask: 0.4915  decode.d2.loss_dice: 0.8532  decode.d3.loss_cls: 0.0544  decode.d3.loss_mask: 0.4903  decode.d3.loss_dice: 0.8487  decode.d4.loss_cls: 0.0543  decode.d4.loss_mask: 0.4935  decode.d4.loss_dice: 0.8739  decode.d5.loss_cls: 0.1032  decode.d5.loss_mask: 0.4918  decode.d5.loss_dice: 0.8482  decode.d6.loss_cls: 0.0541  decode.d6.loss_mask: 0.4988  decode.d6.loss_dice: 0.8449  decode.d7.loss_cls: 0.0557  decode.d7.loss_mask: 0.4932  decode.d7.loss_dice: 0.8476  decode.d8.loss_cls: 0.0538  decode.d8.loss_mask: 0.4973  decode.d8.loss_dice: 0.8706
2023/06/15 21:22:00 - mmengine - INFO - Iter(train) [ 540/3000]  lr: 9.9697e-06  eta: 0:19:14  time: 0.4547  data_time: 0.0043  memory: 8331  grad_norm: 193.4878  loss: 18.8644  decode.loss_cls: 0.0398  decode.loss_mask: 0.4587  decode.loss_dice: 0.8116  decode.d0.loss_cls: 5.9607  decode.d0.loss_mask: 0.4447  decode.d0.loss_dice: 0.8114  decode.d1.loss_cls: 0.0596  decode.d1.loss_mask: 0.4565  decode.d1.loss_dice: 0.7562  decode.d2.loss_cls: 0.0351  decode.d2.loss_mask: 0.4648  decode.d2.loss_dice: 0.7859  decode.d3.loss_cls: 0.0350  decode.d3.loss_mask: 0.4569  decode.d3.loss_dice: 0.7860  decode.d4.loss_cls: 0.0317  decode.d4.loss_mask: 0.4637  decode.d4.loss_dice: 0.7935  decode.d5.loss_cls: 0.0353  decode.d5.loss_mask: 0.4591  decode.d5.loss_dice: 0.8063  decode.d6.loss_cls: 0.0371  decode.d6.loss_mask: 0.4573  decode.d6.loss_dice: 0.8004  decode.d7.loss_cls: 0.0390  decode.d7.loss_mask: 0.4558  decode.d7.loss_dice: 0.8197  decode.d8.loss_cls: 0.0400  decode.d8.loss_mask: 0.4542  decode.d8.loss_dice: 0.8082
2023/06/15 21:22:14 - mmengine - INFO - Iter(train) [ 570/3000]  lr: 9.9680e-06  eta: 0:18:59  time: 0.4569  data_time: 0.0042  memory: 8320  grad_norm: 328.7366  loss: 19.2812  decode.loss_cls: 0.1135  decode.loss_mask: 0.4839  decode.loss_dice: 0.7749  decode.d0.loss_cls: 5.8231  decode.d0.loss_mask: 0.4503  decode.d0.loss_dice: 0.7170  decode.d1.loss_cls: 0.1373  decode.d1.loss_mask: 0.4562  decode.d1.loss_dice: 0.7348  decode.d2.loss_cls: 0.1427  decode.d2.loss_mask: 0.4512  decode.d2.loss_dice: 0.7336  decode.d3.loss_cls: 0.1326  decode.d3.loss_mask: 0.4730  decode.d3.loss_dice: 0.7454  decode.d4.loss_cls: 0.1630  decode.d4.loss_mask: 0.4694  decode.d4.loss_dice: 0.7557  decode.d5.loss_cls: 0.1200  decode.d5.loss_mask: 0.4850  decode.d5.loss_dice: 0.7711  decode.d6.loss_cls: 0.1167  decode.d6.loss_mask: 0.4903  decode.d6.loss_dice: 0.7868  decode.d7.loss_cls: 0.1305  decode.d7.loss_mask: 0.4836  decode.d7.loss_dice: 0.7681  decode.d8.loss_cls: 0.1184  decode.d8.loss_mask: 0.4883  decode.d8.loss_dice: 0.7649
2023/06/15 21:22:27 - mmengine - INFO - Iter(train) [ 600/3000]  lr: 9.9663e-06  eta: 0:18:43  time: 0.4573  data_time: 0.0043  memory: 8331  grad_norm: 364.4572  loss: 20.2060  decode.loss_cls: 0.0482  decode.loss_mask: 0.6043  decode.loss_dice: 0.8263  decode.d0.loss_cls: 5.4342  decode.d0.loss_mask: 0.5836  decode.d0.loss_dice: 0.8360  decode.d1.loss_cls: 0.0504  decode.d1.loss_mask: 0.6103  decode.d1.loss_dice: 0.8094  decode.d2.loss_cls: 0.0401  decode.d2.loss_mask: 0.6084  decode.d2.loss_dice: 0.8233  decode.d3.loss_cls: 0.0465  decode.d3.loss_mask: 0.6127  decode.d3.loss_dice: 0.8222  decode.d4.loss_cls: 0.0439  decode.d4.loss_mask: 0.6251  decode.d4.loss_dice: 0.8323  decode.d5.loss_cls: 0.0421  decode.d5.loss_mask: 0.6160  decode.d5.loss_dice: 0.8351  decode.d6.loss_cls: 0.0475  decode.d6.loss_mask: 0.6113  decode.d6.loss_dice: 0.8282  decode.d7.loss_cls: 0.0485  decode.d7.loss_mask: 0.6019  decode.d7.loss_dice: 0.8307  decode.d8.loss_cls: 0.0513  decode.d8.loss_mask: 0.6040  decode.d8.loss_dice: 0.8320
2023/06/15 21:22:27 - mmengine - INFO - Saving checkpoint at 600 iterations
2023/06/15 21:22:34 - mmengine - INFO - per class results:
2023/06/15 21:22:34 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 97.54 |  98.1 |
|    watermelon_green   | 93.08 | 99.54 |
|    watermelon_white   | 92.22 | 95.03 |
| watermelon_seed_black | 75.98 | 81.84 |
| watermelon_seed_white | 58.34 | 74.21 |
|       Unlabeled       | 34.72 |  35.9 |
+-----------------------+-------+-------+
2023/06/15 21:22:34 - mmengine - INFO - Iter(val) [11/11]    aAcc: 96.9100  mIoU: 75.3100  mAcc: 80.7700  data_time: 0.0075  time: 0.1205
2023/06/15 21:22:34 - mmengine - INFO - The previous best checkpoint D:\ACpycharm\myProjeycs\mmsegmentation-main\tools\work_dirs\AwatermelonMask2Former\best_mIoU_iter_300.pth is removed
2023/06/15 21:22:36 - mmengine - INFO - The best checkpoint with 75.3100 mIoU at 600 iter is saved to best_mIoU_iter_600.pth.
2023/06/15 21:22:50 - mmengine - INFO - Iter(train) [ 630/3000]  lr: 9.9646e-06  eta: 0:18:37  time: 0.4550  data_time: 0.0042  memory: 8331  grad_norm: 304.5350  loss: 18.7752  decode.loss_cls: 0.0345  decode.loss_mask: 0.4928  decode.loss_dice: 0.8251  decode.d0.loss_cls: 5.3338  decode.d0.loss_mask: 0.4656  decode.d0.loss_dice: 0.8058  decode.d1.loss_cls: 0.0591  decode.d1.loss_mask: 0.4936  decode.d1.loss_dice: 0.8115  decode.d2.loss_cls: 0.0378  decode.d2.loss_mask: 0.4873  decode.d2.loss_dice: 0.8228  decode.d3.loss_cls: 0.0337  decode.d3.loss_mask: 0.4878  decode.d3.loss_dice: 0.8102  decode.d4.loss_cls: 0.0305  decode.d4.loss_mask: 0.4961  decode.d4.loss_dice: 0.8292  decode.d5.loss_cls: 0.0365  decode.d5.loss_mask: 0.4870  decode.d5.loss_dice: 0.8236  decode.d6.loss_cls: 0.0342  decode.d6.loss_mask: 0.4902  decode.d6.loss_dice: 0.8250  decode.d7.loss_cls: 0.0337  decode.d7.loss_mask: 0.4926  decode.d7.loss_dice: 0.8417  decode.d8.loss_cls: 0.0354  decode.d8.loss_mask: 0.4940  decode.d8.loss_dice: 0.8241
2023/06/15 21:23:04 - mmengine - INFO - Iter(train) [ 660/3000]  lr: 9.9629e-06  eta: 0:18:21  time: 0.4550  data_time: 0.0042  memory: 8331  grad_norm: 338.4422  loss: 16.6305  decode.loss_cls: 0.0259  decode.loss_mask: 0.4879  decode.loss_dice: 0.6540  decode.d0.loss_cls: 5.0595  decode.d0.loss_mask: 0.4798  decode.d0.loss_dice: 0.6294  decode.d1.loss_cls: 0.0500  decode.d1.loss_mask: 0.4903  decode.d1.loss_dice: 0.6490  decode.d2.loss_cls: 0.0269  decode.d2.loss_mask: 0.4960  decode.d2.loss_dice: 0.6388  decode.d3.loss_cls: 0.0250  decode.d3.loss_mask: 0.4970  decode.d3.loss_dice: 0.6227  decode.d4.loss_cls: 0.0253  decode.d4.loss_mask: 0.5006  decode.d4.loss_dice: 0.6327  decode.d5.loss_cls: 0.0274  decode.d5.loss_mask: 0.4995  decode.d5.loss_dice: 0.6356  decode.d6.loss_cls: 0.0280  decode.d6.loss_mask: 0.4972  decode.d6.loss_dice: 0.6400  decode.d7.loss_cls: 0.0279  decode.d7.loss_mask: 0.4897  decode.d7.loss_dice: 0.6373  decode.d8.loss_cls: 0.0259  decode.d8.loss_mask: 0.4886  decode.d8.loss_dice: 0.6426
2023/06/15 21:23:17 - mmengine - INFO - Iter(train) [ 690/3000]  lr: 9.9612e-06  eta: 0:18:05  time: 0.4539  data_time: 0.0038  memory: 8331  grad_norm: 253.5505  loss: 18.3415  decode.loss_cls: 0.0716  decode.loss_mask: 0.4488  decode.loss_dice: 0.8488  decode.d0.loss_cls: 4.8072  decode.d0.loss_mask: 0.4366  decode.d0.loss_dice: 0.8461  decode.d1.loss_cls: 0.0570  decode.d1.loss_mask: 0.4433  decode.d1.loss_dice: 0.8509  decode.d2.loss_cls: 0.0638  decode.d2.loss_mask: 0.4403  decode.d2.loss_dice: 0.8376  decode.d3.loss_cls: 0.0672  decode.d3.loss_mask: 0.4445  decode.d3.loss_dice: 0.8657  decode.d4.loss_cls: 0.0710  decode.d4.loss_mask: 0.4416  decode.d4.loss_dice: 0.8484  decode.d5.loss_cls: 0.0676  decode.d5.loss_mask: 0.4429  decode.d5.loss_dice: 0.8423  decode.d6.loss_cls: 0.0707  decode.d6.loss_mask: 0.4491  decode.d6.loss_dice: 0.8494  decode.d7.loss_cls: 0.0691  decode.d7.loss_mask: 0.4520  decode.d7.loss_dice: 0.8509  decode.d8.loss_cls: 0.0660  decode.d8.loss_mask: 0.4527  decode.d8.loss_dice: 0.8386
2023/06/15 21:23:31 - mmengine - INFO - Iter(train) [ 720/3000]  lr: 9.9595e-06  eta: 0:17:50  time: 0.4550  data_time: 0.0042  memory: 8331  grad_norm: 375.0836  loss: 17.8498  decode.loss_cls: 0.0888  decode.loss_mask: 0.5240  decode.loss_dice: 0.7055  decode.d0.loss_cls: 4.6856  decode.d0.loss_mask: 0.5173  decode.d0.loss_dice: 0.7259  decode.d1.loss_cls: 0.1064  decode.d1.loss_mask: 0.5251  decode.d1.loss_dice: 0.7175  decode.d2.loss_cls: 0.0988  decode.d2.loss_mask: 0.5341  decode.d2.loss_dice: 0.7058  decode.d3.loss_cls: 0.0950  decode.d3.loss_mask: 0.5365  decode.d3.loss_dice: 0.7237  decode.d4.loss_cls: 0.1026  decode.d4.loss_mask: 0.5255  decode.d4.loss_dice: 0.7140  decode.d5.loss_cls: 0.0963  decode.d5.loss_mask: 0.5278  decode.d5.loss_dice: 0.7052  decode.d6.loss_cls: 0.0880  decode.d6.loss_mask: 0.5245  decode.d6.loss_dice: 0.7024  decode.d7.loss_cls: 0.0416  decode.d7.loss_mask: 0.5292  decode.d7.loss_dice: 0.7187  decode.d8.loss_cls: 0.0358  decode.d8.loss_mask: 0.5286  decode.d8.loss_dice: 0.7198
2023/06/15 21:23:45 - mmengine - INFO - Iter(train) [ 750/3000]  lr: 9.9579e-06  eta: 0:17:34  time: 0.4552  data_time: 0.0042  memory: 8331  grad_norm: 186.6264  loss: 16.1495  decode.loss_cls: 0.0148  decode.loss_mask: 0.4773  decode.loss_dice: 0.6849  decode.d0.loss_cls: 4.3739  decode.d0.loss_mask: 0.4692  decode.d0.loss_dice: 0.6830  decode.d1.loss_cls: 0.0412  decode.d1.loss_mask: 0.4754  decode.d1.loss_dice: 0.6737  decode.d2.loss_cls: 0.0187  decode.d2.loss_mask: 0.4786  decode.d2.loss_dice: 0.6714  decode.d3.loss_cls: 0.0170  decode.d3.loss_mask: 0.4809  decode.d3.loss_dice: 0.6797  decode.d4.loss_cls: 0.0169  decode.d4.loss_mask: 0.4812  decode.d4.loss_dice: 0.6834  decode.d5.loss_cls: 0.0179  decode.d5.loss_mask: 0.4759  decode.d5.loss_dice: 0.6821  decode.d6.loss_cls: 0.0172  decode.d6.loss_mask: 0.4792  decode.d6.loss_dice: 0.6801  decode.d7.loss_cls: 0.0170  decode.d7.loss_mask: 0.4835  decode.d7.loss_dice: 0.6935  decode.d8.loss_cls: 0.0163  decode.d8.loss_mask: 0.4757  decode.d8.loss_dice: 0.6898
2023/06/15 21:23:58 - mmengine - INFO - Iter(train) [ 780/3000]  lr: 9.9562e-06  eta: 0:17:19  time: 0.4548  data_time: 0.0041  memory: 8331  grad_norm: 317.9793  loss: 14.8960  decode.loss_cls: 0.0218  decode.loss_mask: 0.3964  decode.loss_dice: 0.6572  decode.d0.loss_cls: 4.1440  decode.d0.loss_mask: 0.3850  decode.d0.loss_dice: 0.6711  decode.d1.loss_cls: 0.0440  decode.d1.loss_mask: 0.3930  decode.d1.loss_dice: 0.6677  decode.d2.loss_cls: 0.0264  decode.d2.loss_mask: 0.3985  decode.d2.loss_dice: 0.6637  decode.d3.loss_cls: 0.0184  decode.d3.loss_mask: 0.3967  decode.d3.loss_dice: 0.6760  decode.d4.loss_cls: 0.0190  decode.d4.loss_mask: 0.3988  decode.d4.loss_dice: 0.6563  decode.d5.loss_cls: 0.0209  decode.d5.loss_mask: 0.3930  decode.d5.loss_dice: 0.6497  decode.d6.loss_cls: 0.0185  decode.d6.loss_mask: 0.3939  decode.d6.loss_dice: 0.6521  decode.d7.loss_cls: 0.0187  decode.d7.loss_mask: 0.3980  decode.d7.loss_dice: 0.6492  decode.d8.loss_cls: 0.0212  decode.d8.loss_mask: 0.3962  decode.d8.loss_dice: 0.6506
2023/06/15 21:24:12 - mmengine - INFO - Iter(train) [ 810/3000]  lr: 9.9545e-06  eta: 0:17:04  time: 0.4572  data_time: 0.0043  memory: 8331  grad_norm: 399.8589  loss: 16.0849  decode.loss_cls: 0.0419  decode.loss_mask: 0.4898  decode.loss_dice: 0.6766  decode.d0.loss_cls: 4.0524  decode.d0.loss_mask: 0.4940  decode.d0.loss_dice: 0.6675  decode.d1.loss_cls: 0.0632  decode.d1.loss_mask: 0.4981  decode.d1.loss_dice: 0.6717  decode.d2.loss_cls: 0.0428  decode.d2.loss_mask: 0.4908  decode.d2.loss_dice: 0.6747  decode.d3.loss_cls: 0.0359  decode.d3.loss_mask: 0.4936  decode.d3.loss_dice: 0.6764  decode.d4.loss_cls: 0.0411  decode.d4.loss_mask: 0.4903  decode.d4.loss_dice: 0.6818  decode.d5.loss_cls: 0.0364  decode.d5.loss_mask: 0.4887  decode.d5.loss_dice: 0.6707  decode.d6.loss_cls: 0.0325  decode.d6.loss_mask: 0.4914  decode.d6.loss_dice: 0.6688  decode.d7.loss_cls: 0.0369  decode.d7.loss_mask: 0.4868  decode.d7.loss_dice: 0.6737  decode.d8.loss_cls: 0.0402  decode.d8.loss_mask: 0.4947  decode.d8.loss_dice: 0.6815
2023/06/15 21:24:26 - mmengine - INFO - Iter(train) [ 840/3000]  lr: 9.9528e-06  eta: 0:16:49  time: 0.4556  data_time: 0.0042  memory: 8331  grad_norm: 590.2980  loss: 17.4946  decode.loss_cls: 0.0256  decode.loss_mask: 0.6090  decode.loss_dice: 0.7519  decode.d0.loss_cls: 3.7162  decode.d0.loss_mask: 0.6162  decode.d0.loss_dice: 0.7365  decode.d1.loss_cls: 0.0461  decode.d1.loss_mask: 0.6162  decode.d1.loss_dice: 0.7361  decode.d2.loss_cls: 0.0251  decode.d2.loss_mask: 0.6095  decode.d2.loss_dice: 0.7224  decode.d3.loss_cls: 0.0201  decode.d3.loss_mask: 0.6124  decode.d3.loss_dice: 0.7315  decode.d4.loss_cls: 0.0212  decode.d4.loss_mask: 0.6130  decode.d4.loss_dice: 0.7483  decode.d5.loss_cls: 0.0224  decode.d5.loss_mask: 0.6235  decode.d5.loss_dice: 0.7459  decode.d6.loss_cls: 0.0226  decode.d6.loss_mask: 0.6189  decode.d6.loss_dice: 0.7404  decode.d7.loss_cls: 0.0258  decode.d7.loss_mask: 0.6130  decode.d7.loss_dice: 0.7321  decode.d8.loss_cls: 0.0248  decode.d8.loss_mask: 0.6193  decode.d8.loss_dice: 0.7487
2023/06/15 21:24:40 - mmengine - INFO - Iter(train) [ 870/3000]  lr: 9.9511e-06  eta: 0:16:34  time: 0.4566  data_time: 0.0043  memory: 8331  grad_norm: 153.1080  loss: 14.9710  decode.loss_cls: 0.0238  decode.loss_mask: 0.4651  decode.loss_dice: 0.6516  decode.d0.loss_cls: 3.4766  decode.d0.loss_mask: 0.4515  decode.d0.loss_dice: 0.6623  decode.d1.loss_cls: 0.0376  decode.d1.loss_mask: 0.4585  decode.d1.loss_dice: 0.6573  decode.d2.loss_cls: 0.0176  decode.d2.loss_mask: 0.4600  decode.d2.loss_dice: 0.6696  decode.d3.loss_cls: 0.0176  decode.d3.loss_mask: 0.4618  decode.d3.loss_dice: 0.6770  decode.d4.loss_cls: 0.0201  decode.d4.loss_mask: 0.4553  decode.d4.loss_dice: 0.6692  decode.d5.loss_cls: 0.0249  decode.d5.loss_mask: 0.4646  decode.d5.loss_dice: 0.6598  decode.d6.loss_cls: 0.0210  decode.d6.loss_mask: 0.4623  decode.d6.loss_dice: 0.6782  decode.d7.loss_cls: 0.0230  decode.d7.loss_mask: 0.4667  decode.d7.loss_dice: 0.6774  decode.d8.loss_cls: 0.0262  decode.d8.loss_mask: 0.4674  decode.d8.loss_dice: 0.6670
2023/06/15 21:24:53 - mmengine - INFO - Iter(train) [ 900/3000]  lr: 9.9494e-06  eta: 0:16:20  time: 0.4572  data_time: 0.0048  memory: 8331  grad_norm: 560.0358  loss: 15.8061  decode.loss_cls: 0.0653  decode.loss_mask: 0.5634  decode.loss_dice: 0.6533  decode.d0.loss_cls: 3.2459  decode.d0.loss_mask: 0.5403  decode.d0.loss_dice: 0.6328  decode.d1.loss_cls: 0.0521  decode.d1.loss_mask: 0.5564  decode.d1.loss_dice: 0.6443  decode.d2.loss_cls: 0.0508  decode.d2.loss_mask: 0.5574  decode.d2.loss_dice: 0.6580  decode.d3.loss_cls: 0.0582  decode.d3.loss_mask: 0.5512  decode.d3.loss_dice: 0.6426  decode.d4.loss_cls: 0.0510  decode.d4.loss_mask: 0.5578  decode.d4.loss_dice: 0.6519  decode.d5.loss_cls: 0.0510  decode.d5.loss_mask: 0.5541  decode.d5.loss_dice: 0.6542  decode.d6.loss_cls: 0.0563  decode.d6.loss_mask: 0.5532  decode.d6.loss_dice: 0.6500  decode.d7.loss_cls: 0.0556  decode.d7.loss_mask: 0.5604  decode.d7.loss_dice: 0.6606  decode.d8.loss_cls: 0.0567  decode.d8.loss_mask: 0.5599  decode.d8.loss_dice: 0.6614
2023/06/15 21:24:53 - mmengine - INFO - Saving checkpoint at 900 iterations
2023/06/15 21:24:59 - mmengine - INFO - per class results:
2023/06/15 21:24:59 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 97.74 | 98.22 |
|    watermelon_green   | 95.66 | 98.07 |
|    watermelon_white   | 91.87 | 97.51 |
| watermelon_seed_black |  84.7 | 93.84 |
| watermelon_seed_white | 66.96 | 87.67 |
|       Unlabeled       | 49.59 | 53.66 |
+-----------------------+-------+-------+
2023/06/15 21:24:59 - mmengine - INFO - Iter(val) [11/11]    aAcc: 97.6900  mIoU: 81.0900  mAcc: 88.1600  data_time: 0.0042  time: 0.1116
2023/06/15 21:24:59 - mmengine - INFO - The previous best checkpoint D:\ACpycharm\myProjeycs\mmsegmentation-main\tools\work_dirs\AwatermelonMask2Former\best_mIoU_iter_600.pth is removed
2023/06/15 21:25:02 - mmengine - INFO - The best checkpoint with 81.0900 mIoU at 900 iter is saved to best_mIoU_iter_900.pth.
2023/06/15 21:25:15 - mmengine - INFO - Iter(train) [ 930/3000]  lr: 9.9477e-06  eta: 0:16:10  time: 0.4603  data_time: 0.0042  memory: 8331  grad_norm: 405.7894  loss: 16.9525  decode.loss_cls: 0.0232  decode.loss_mask: 0.5406  decode.loss_dice: 0.8217  decode.d0.loss_cls: 2.9365  decode.d0.loss_mask: 0.5263  decode.d0.loss_dice: 0.8399  decode.d1.loss_cls: 0.0374  decode.d1.loss_mask: 0.5347  decode.d1.loss_dice: 0.8303  decode.d2.loss_cls: 0.0255  decode.d2.loss_mask: 0.5432  decode.d2.loss_dice: 0.8322  decode.d3.loss_cls: 0.0205  decode.d3.loss_mask: 0.5457  decode.d3.loss_dice: 0.8389  decode.d4.loss_cls: 0.0215  decode.d4.loss_mask: 0.5472  decode.d4.loss_dice: 0.8269  decode.d5.loss_cls: 0.0233  decode.d5.loss_mask: 0.5393  decode.d5.loss_dice: 0.8239  decode.d6.loss_cls: 0.0266  decode.d6.loss_mask: 0.5401  decode.d6.loss_dice: 0.8365  decode.d7.loss_cls: 0.0259  decode.d7.loss_mask: 0.5430  decode.d7.loss_dice: 0.8303  decode.d8.loss_cls: 0.1013  decode.d8.loss_mask: 0.5478  decode.d8.loss_dice: 0.8223
2023/06/15 21:25:29 - mmengine - INFO - Iter(train) [ 960/3000]  lr: 9.9460e-06  eta: 0:15:55  time: 0.4546  data_time: 0.0042  memory: 8331  grad_norm: 288.5290  loss: 15.4123  decode.loss_cls: 0.0190  decode.loss_mask: 0.5536  decode.loss_dice: 0.6979  decode.d0.loss_cls: 2.7755  decode.d0.loss_mask: 0.5401  decode.d0.loss_dice: 0.7007  decode.d1.loss_cls: 0.0330  decode.d1.loss_mask: 0.5573  decode.d1.loss_dice: 0.6832  decode.d2.loss_cls: 0.0261  decode.d2.loss_mask: 0.5537  decode.d2.loss_dice: 0.6877  decode.d3.loss_cls: 0.0326  decode.d3.loss_mask: 0.5514  decode.d3.loss_dice: 0.6784  decode.d4.loss_cls: 0.0305  decode.d4.loss_mask: 0.5563  decode.d4.loss_dice: 0.6864  decode.d5.loss_cls: 0.0288  decode.d5.loss_mask: 0.5542  decode.d5.loss_dice: 0.6749  decode.d6.loss_cls: 0.0215  decode.d6.loss_mask: 0.5566  decode.d6.loss_dice: 0.6878  decode.d7.loss_cls: 0.0217  decode.d7.loss_mask: 0.5475  decode.d7.loss_dice: 0.6876  decode.d8.loss_cls: 0.0245  decode.d8.loss_mask: 0.5541  decode.d8.loss_dice: 0.6895
2023/06/15 21:25:43 - mmengine - INFO - Iter(train) [ 990/3000]  lr: 9.9444e-06  eta: 0:15:40  time: 0.4556  data_time: 0.0043  memory: 8331  grad_norm: 589.1208  loss: 13.2737  decode.loss_cls: 0.0389  decode.loss_mask: 0.3792  decode.loss_dice: 0.6116  decode.d0.loss_cls: 2.5979  decode.d0.loss_mask: 0.3754  decode.d0.loss_dice: 0.6477  decode.d1.loss_cls: 0.0437  decode.d1.loss_mask: 0.3836  decode.d1.loss_dice: 0.6428  decode.d2.loss_cls: 0.1010  decode.d2.loss_mask: 0.3792  decode.d2.loss_dice: 0.6237  decode.d3.loss_cls: 0.1127  decode.d3.loss_mask: 0.3826  decode.d3.loss_dice: 0.6143  decode.d4.loss_cls: 0.0851  decode.d4.loss_mask: 0.3851  decode.d4.loss_dice: 0.6336  decode.d5.loss_cls: 0.1069  decode.d5.loss_mask: 0.3817  decode.d5.loss_dice: 0.6127  decode.d6.loss_cls: 0.0455  decode.d6.loss_mask: 0.3814  decode.d6.loss_dice: 0.6195  decode.d7.loss_cls: 0.0418  decode.d7.loss_mask: 0.3855  decode.d7.loss_dice: 0.6211  decode.d8.loss_cls: 0.0418  decode.d8.loss_mask: 0.3800  decode.d8.loss_dice: 0.6177
2023/06/15 21:25:47 - mmengine - INFO - Exp name: AwatermelonMask2Former_20230615_211716
2023/06/15 21:25:56 - mmengine - INFO - Iter(train) [1020/3000]  lr: 9.9427e-06  eta: 0:15:25  time: 0.4550  data_time: 0.0041  memory: 8331  grad_norm: 214.5087  loss: 12.2821  decode.loss_cls: 0.0426  decode.loss_mask: 0.3478  decode.loss_dice: 0.5789  decode.d0.loss_cls: 2.3896  decode.d0.loss_mask: 0.3445  decode.d0.loss_dice: 0.5974  decode.d1.loss_cls: 0.0451  decode.d1.loss_mask: 0.3493  decode.d1.loss_dice: 0.5984  decode.d2.loss_cls: 0.0784  decode.d2.loss_mask: 0.3534  decode.d2.loss_dice: 0.5814  decode.d3.loss_cls: 0.0714  decode.d3.loss_mask: 0.3545  decode.d3.loss_dice: 0.5933  decode.d4.loss_cls: 0.0525  decode.d4.loss_mask: 0.3514  decode.d4.loss_dice: 0.5911  decode.d5.loss_cls: 0.0490  decode.d5.loss_mask: 0.3529  decode.d5.loss_dice: 0.5874  decode.d6.loss_cls: 0.0475  decode.d6.loss_mask: 0.3534  decode.d6.loss_dice: 0.5890  decode.d7.loss_cls: 0.0446  decode.d7.loss_mask: 0.3522  decode.d7.loss_dice: 0.5907  decode.d8.loss_cls: 0.0448  decode.d8.loss_mask: 0.3517  decode.d8.loss_dice: 0.5979
2023/06/15 21:26:10 - mmengine - INFO - Iter(train) [1050/3000]  lr: 9.9410e-06  eta: 0:15:11  time: 0.4557  data_time: 0.0043  memory: 8331  grad_norm: 379.9078  loss: 14.6181  decode.loss_cls: 0.0279  decode.loss_mask: 0.4032  decode.loss_dice: 0.7909  decode.d0.loss_cls: 2.1111  decode.d0.loss_mask: 0.4028  decode.d0.loss_dice: 0.7889  decode.d1.loss_cls: 0.0360  decode.d1.loss_mask: 0.4056  decode.d1.loss_dice: 0.7888  decode.d2.loss_cls: 0.0872  decode.d2.loss_mask: 0.4045  decode.d2.loss_dice: 0.7833  decode.d3.loss_cls: 0.0896  decode.d3.loss_mask: 0.4017  decode.d3.loss_dice: 0.7921  decode.d4.loss_cls: 0.0410  decode.d4.loss_mask: 0.4048  decode.d4.loss_dice: 0.7823  decode.d5.loss_cls: 0.0799  decode.d5.loss_mask: 0.4034  decode.d5.loss_dice: 0.7898  decode.d6.loss_cls: 0.0900  decode.d6.loss_mask: 0.4068  decode.d6.loss_dice: 0.7804  decode.d7.loss_cls: 0.0764  decode.d7.loss_mask: 0.4075  decode.d7.loss_dice: 0.7722  decode.d8.loss_cls: 0.0870  decode.d8.loss_mask: 0.4039  decode.d8.loss_dice: 0.7791
2023/06/15 21:26:24 - mmengine - INFO - Iter(train) [1080/3000]  lr: 9.9393e-06  eta: 0:14:56  time: 0.4543  data_time: 0.0040  memory: 8331  grad_norm: 820.4009  loss: 14.1124  decode.loss_cls: 0.0691  decode.loss_mask: 0.4764  decode.loss_dice: 0.6966  decode.d0.loss_cls: 1.9631  decode.d0.loss_mask: 0.4699  decode.d0.loss_dice: 0.7121  decode.d1.loss_cls: 0.0397  decode.d1.loss_mask: 0.4763  decode.d1.loss_dice: 0.6849  decode.d2.loss_cls: 0.0283  decode.d2.loss_mask: 0.4775  decode.d2.loss_dice: 0.6860  decode.d3.loss_cls: 0.0248  decode.d3.loss_mask: 0.4759  decode.d3.loss_dice: 0.7016  decode.d4.loss_cls: 0.0314  decode.d4.loss_mask: 0.4824  decode.d4.loss_dice: 0.7141  decode.d5.loss_cls: 0.0293  decode.d5.loss_mask: 0.4771  decode.d5.loss_dice: 0.6883  decode.d6.loss_cls: 0.0682  decode.d6.loss_mask: 0.4766  decode.d6.loss_dice: 0.6911  decode.d7.loss_cls: 0.0735  decode.d7.loss_mask: 0.4740  decode.d7.loss_dice: 0.6988  decode.d8.loss_cls: 0.0377  decode.d8.loss_mask: 0.4778  decode.d8.loss_dice: 0.7099
2023/06/15 21:26:37 - mmengine - INFO - Iter(train) [1110/3000]  lr: 9.9376e-06  eta: 0:14:42  time: 0.4572  data_time: 0.0040  memory: 8320  grad_norm: 415.3615  loss: 12.9117  decode.loss_cls: 0.1085  decode.loss_mask: 0.3659  decode.loss_dice: 0.6575  decode.d0.loss_cls: 1.8322  decode.d0.loss_mask: 0.3498  decode.d0.loss_dice: 0.6892  decode.d1.loss_cls: 0.0989  decode.d1.loss_mask: 0.3598  decode.d1.loss_dice: 0.6595  decode.d2.loss_cls: 0.1016  decode.d2.loss_mask: 0.3640  decode.d2.loss_dice: 0.6598  decode.d3.loss_cls: 0.0270  decode.d3.loss_mask: 0.3673  decode.d3.loss_dice: 0.6778  decode.d4.loss_cls: 0.0943  decode.d4.loss_mask: 0.3679  decode.d4.loss_dice: 0.6783  decode.d5.loss_cls: 0.0876  decode.d5.loss_mask: 0.3664  decode.d5.loss_dice: 0.6589  decode.d6.loss_cls: 0.0966  decode.d6.loss_mask: 0.3619  decode.d6.loss_dice: 0.6776  decode.d7.loss_cls: 0.0287  decode.d7.loss_mask: 0.3678  decode.d7.loss_dice: 0.6807  decode.d8.loss_cls: 0.0817  decode.d8.loss_mask: 0.3617  decode.d8.loss_dice: 0.6828
2023/06/15 21:26:51 - mmengine - INFO - Iter(train) [1140/3000]  lr: 9.9359e-06  eta: 0:14:27  time: 0.4590  data_time: 0.0044  memory: 8331  grad_norm: 97.6137  loss: 11.0604  decode.loss_cls: 0.0487  decode.loss_mask: 0.3831  decode.loss_dice: 0.5238  decode.d0.loss_cls: 1.6221  decode.d0.loss_mask: 0.3795  decode.d0.loss_dice: 0.5266  decode.d1.loss_cls: 0.0507  decode.d1.loss_mask: 0.3863  decode.d1.loss_dice: 0.5228  decode.d2.loss_cls: 0.0425  decode.d2.loss_mask: 0.3796  decode.d2.loss_dice: 0.5297  decode.d3.loss_cls: 0.0399  decode.d3.loss_mask: 0.3834  decode.d3.loss_dice: 0.5310  decode.d4.loss_cls: 0.0351  decode.d4.loss_mask: 0.3840  decode.d4.loss_dice: 0.5250  decode.d5.loss_cls: 0.0395  decode.d5.loss_mask: 0.3806  decode.d5.loss_dice: 0.5210  decode.d6.loss_cls: 0.0292  decode.d6.loss_mask: 0.3807  decode.d6.loss_dice: 0.5222  decode.d7.loss_cls: 0.0309  decode.d7.loss_mask: 0.3877  decode.d7.loss_dice: 0.5183  decode.d8.loss_cls: 0.0336  decode.d8.loss_mask: 0.3875  decode.d8.loss_dice: 0.5355
2023/06/15 21:27:05 - mmengine - INFO - Iter(train) [1170/3000]  lr: 9.9342e-06  eta: 0:14:13  time: 0.4549  data_time: 0.0042  memory: 8331  grad_norm: 156.0597  loss: 11.8916  decode.loss_cls: 0.0393  decode.loss_mask: 0.3980  decode.loss_dice: 0.6083  decode.d0.loss_cls: 1.4675  decode.d0.loss_mask: 0.4016  decode.d0.loss_dice: 0.6115  decode.d1.loss_cls: 0.0491  decode.d1.loss_mask: 0.3978  decode.d1.loss_dice: 0.6241  decode.d2.loss_cls: 0.0474  decode.d2.loss_mask: 0.4023  decode.d2.loss_dice: 0.6075  decode.d3.loss_cls: 0.0345  decode.d3.loss_mask: 0.3998  decode.d3.loss_dice: 0.6017  decode.d4.loss_cls: 0.0420  decode.d4.loss_mask: 0.3986  decode.d4.loss_dice: 0.6013  decode.d5.loss_cls: 0.0437  decode.d5.loss_mask: 0.4016  decode.d5.loss_dice: 0.5931  decode.d6.loss_cls: 0.0295  decode.d6.loss_mask: 0.3987  decode.d6.loss_dice: 0.5906  decode.d7.loss_cls: 0.0399  decode.d7.loss_mask: 0.4036  decode.d7.loss_dice: 0.6146  decode.d8.loss_cls: 0.0401  decode.d8.loss_mask: 0.3946  decode.d8.loss_dice: 0.6092
2023/06/15 21:27:19 - mmengine - INFO - Iter(train) [1200/3000]  lr: 9.9325e-06  eta: 0:13:58  time: 0.4547  data_time: 0.0040  memory: 8320  grad_norm: 131.8097  loss: 11.0077  decode.loss_cls: 0.0988  decode.loss_mask: 0.3580  decode.loss_dice: 0.5485  decode.d0.loss_cls: 1.3167  decode.d0.loss_mask: 0.3462  decode.d0.loss_dice: 0.5574  decode.d1.loss_cls: 0.0598  decode.d1.loss_mask: 0.3540  decode.d1.loss_dice: 0.5255  decode.d2.loss_cls: 0.0572  decode.d2.loss_mask: 0.3545  decode.d2.loss_dice: 0.5344  decode.d3.loss_cls: 0.1062  decode.d3.loss_mask: 0.3515  decode.d3.loss_dice: 0.5524  decode.d4.loss_cls: 0.0512  decode.d4.loss_mask: 0.3530  decode.d4.loss_dice: 0.5442  decode.d5.loss_cls: 0.0715  decode.d5.loss_mask: 0.3541  decode.d5.loss_dice: 0.5383  decode.d6.loss_cls: 0.0933  decode.d6.loss_mask: 0.3538  decode.d6.loss_dice: 0.5442  decode.d7.loss_cls: 0.0969  decode.d7.loss_mask: 0.3518  decode.d7.loss_dice: 0.5463  decode.d8.loss_cls: 0.0910  decode.d8.loss_mask: 0.3516  decode.d8.loss_dice: 0.5450
2023/06/15 21:27:19 - mmengine - INFO - Saving checkpoint at 1200 iterations
2023/06/15 21:27:25 - mmengine - INFO - per class results:
2023/06/15 21:27:25 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 97.96 | 98.42 |
|    watermelon_green   | 95.52 |  98.9 |
|    watermelon_white   |  93.2 | 97.81 |
| watermelon_seed_black | 84.94 | 91.07 |
| watermelon_seed_white | 68.32 | 82.68 |
|       Unlabeled       | 44.81 |  46.6 |
+-----------------------+-------+-------+
2023/06/15 21:27:25 - mmengine - INFO - Iter(val) [11/11]    aAcc: 97.8200  mIoU: 80.7900  mAcc: 85.9100  data_time: 0.0040  time: 0.1060
2023/06/15 21:27:38 - mmengine - INFO - Iter(train) [1230/3000]  lr: 9.9308e-06  eta: 0:13:44  time: 0.4576  data_time: 0.0043  memory: 8331  grad_norm: 148.7798  loss: 12.0674  decode.loss_cls: 0.0375  decode.loss_mask: 0.4283  decode.loss_dice: 0.6193  decode.d0.loss_cls: 1.1758  decode.d0.loss_mask: 0.4319  decode.d0.loss_dice: 0.6151  decode.d1.loss_cls: 0.0589  decode.d1.loss_mask: 0.4247  decode.d1.loss_dice: 0.6219  decode.d2.loss_cls: 0.0420  decode.d2.loss_mask: 0.4284  decode.d2.loss_dice: 0.6396  decode.d3.loss_cls: 0.0324  decode.d3.loss_mask: 0.4241  decode.d3.loss_dice: 0.6232  decode.d4.loss_cls: 0.0387  decode.d4.loss_mask: 0.4258  decode.d4.loss_dice: 0.6377  decode.d5.loss_cls: 0.0318  decode.d5.loss_mask: 0.4249  decode.d5.loss_dice: 0.6230  decode.d6.loss_cls: 0.0321  decode.d6.loss_mask: 0.4274  decode.d6.loss_dice: 0.6327  decode.d7.loss_cls: 0.0307  decode.d7.loss_mask: 0.4338  decode.d7.loss_dice: 0.6333  decode.d8.loss_cls: 0.0329  decode.d8.loss_mask: 0.4300  decode.d8.loss_dice: 0.6294
2023/06/15 21:27:52 - mmengine - INFO - Iter(train) [1260/3000]  lr: 9.9292e-06  eta: 0:13:29  time: 0.4551  data_time: 0.0044  memory: 8331  grad_norm: 195.3382  loss: 11.7558  decode.loss_cls: 0.1161  decode.loss_mask: 0.3995  decode.loss_dice: 0.6065  decode.d0.loss_cls: 1.1120  decode.d0.loss_mask: 0.4052  decode.d0.loss_dice: 0.5791  decode.d1.loss_cls: 0.1069  decode.d1.loss_mask: 0.4026  decode.d1.loss_dice: 0.5432  decode.d2.loss_cls: 0.0471  decode.d2.loss_mask: 0.3992  decode.d2.loss_dice: 0.6117  decode.d3.loss_cls: 0.0296  decode.d3.loss_mask: 0.4008  decode.d3.loss_dice: 0.5965  decode.d4.loss_cls: 0.0916  decode.d4.loss_mask: 0.4041  decode.d4.loss_dice: 0.5703  decode.d5.loss_cls: 0.0951  decode.d5.loss_mask: 0.4045  decode.d5.loss_dice: 0.5539  decode.d6.loss_cls: 0.0919  decode.d6.loss_mask: 0.4071  decode.d6.loss_dice: 0.5915  decode.d7.loss_cls: 0.0885  decode.d7.loss_mask: 0.4014  decode.d7.loss_dice: 0.5833  decode.d8.loss_cls: 0.1265  decode.d8.loss_mask: 0.3990  decode.d8.loss_dice: 0.5911
2023/06/15 21:28:06 - mmengine - INFO - Iter(train) [1290/3000]  lr: 9.9275e-06  eta: 0:13:15  time: 0.4548  data_time: 0.0042  memory: 8331  grad_norm: 201.1538  loss: 12.1479  decode.loss_cls: 0.1116  decode.loss_mask: 0.3444  decode.loss_dice: 0.6890  decode.d0.loss_cls: 0.9250  decode.d0.loss_mask: 0.3479  decode.d0.loss_dice: 0.6837  decode.d1.loss_cls: 0.0838  decode.d1.loss_mask: 0.3480  decode.d1.loss_dice: 0.7034  decode.d2.loss_cls: 0.0644  decode.d2.loss_mask: 0.3465  decode.d2.loss_dice: 0.6836  decode.d3.loss_cls: 0.0879  decode.d3.loss_mask: 0.3510  decode.d3.loss_dice: 0.6822  decode.d4.loss_cls: 0.0841  decode.d4.loss_mask: 0.3467  decode.d4.loss_dice: 0.6744  decode.d5.loss_cls: 0.1034  decode.d5.loss_mask: 0.3487  decode.d5.loss_dice: 0.6918  decode.d6.loss_cls: 0.0884  decode.d6.loss_mask: 0.3471  decode.d6.loss_dice: 0.7025  decode.d7.loss_cls: 0.0966  decode.d7.loss_mask: 0.3453  decode.d7.loss_dice: 0.7010  decode.d8.loss_cls: 0.1103  decode.d8.loss_mask: 0.3486  decode.d8.loss_dice: 0.7066
2023/06/15 21:28:19 - mmengine - INFO - Iter(train) [1320/3000]  lr: 9.9258e-06  eta: 0:13:01  time: 0.4555  data_time: 0.0042  memory: 8331  grad_norm: 165.2731  loss: 9.9181  decode.loss_cls: 0.0223  decode.loss_mask: 0.3735  decode.loss_dice: 0.5155  decode.d0.loss_cls: 0.8426  decode.d0.loss_mask: 0.3686  decode.d0.loss_dice: 0.5177  decode.d1.loss_cls: 0.0275  decode.d1.loss_mask: 0.3760  decode.d1.loss_dice: 0.5209  decode.d2.loss_cls: 0.0145  decode.d2.loss_mask: 0.3725  decode.d2.loss_dice: 0.5151  decode.d3.loss_cls: 0.0142  decode.d3.loss_mask: 0.3742  decode.d3.loss_dice: 0.5126  decode.d4.loss_cls: 0.0170  decode.d4.loss_mask: 0.3736  decode.d4.loss_dice: 0.5059  decode.d5.loss_cls: 0.0202  decode.d5.loss_mask: 0.3766  decode.d5.loss_dice: 0.5180  decode.d6.loss_cls: 0.0189  decode.d6.loss_mask: 0.3714  decode.d6.loss_dice: 0.5150  decode.d7.loss_cls: 0.0208  decode.d7.loss_mask: 0.3749  decode.d7.loss_dice: 0.5200  decode.d8.loss_cls: 0.0236  decode.d8.loss_mask: 0.3748  decode.d8.loss_dice: 0.5196
2023/06/15 21:28:33 - mmengine - INFO - Iter(train) [1350/3000]  lr: 9.9241e-06  eta: 0:12:47  time: 0.4562  data_time: 0.0043  memory: 8331  grad_norm: 163.8255  loss: 9.7782  decode.loss_cls: 0.0193  decode.loss_mask: 0.4012  decode.loss_dice: 0.4845  decode.d0.loss_cls: 0.7748  decode.d0.loss_mask: 0.4006  decode.d0.loss_dice: 0.4801  decode.d1.loss_cls: 0.0284  decode.d1.loss_mask: 0.3973  decode.d1.loss_dice: 0.4902  decode.d2.loss_cls: 0.0198  decode.d2.loss_mask: 0.3975  decode.d2.loss_dice: 0.4959  decode.d3.loss_cls: 0.0176  decode.d3.loss_mask: 0.3955  decode.d3.loss_dice: 0.4864  decode.d4.loss_cls: 0.0146  decode.d4.loss_mask: 0.3988  decode.d4.loss_dice: 0.4880  decode.d5.loss_cls: 0.0169  decode.d5.loss_mask: 0.3960  decode.d5.loss_dice: 0.4799  decode.d6.loss_cls: 0.0168  decode.d6.loss_mask: 0.3981  decode.d6.loss_dice: 0.4958  decode.d7.loss_cls: 0.0159  decode.d7.loss_mask: 0.3959  decode.d7.loss_dice: 0.4816  decode.d8.loss_cls: 0.0180  decode.d8.loss_mask: 0.3978  decode.d8.loss_dice: 0.4749
2023/06/15 21:28:47 - mmengine - INFO - Iter(train) [1380/3000]  lr: 9.9224e-06  eta: 0:12:32  time: 0.4553  data_time: 0.0041  memory: 8331  grad_norm: 191.2493  loss: 10.7896  decode.loss_cls: 0.0222  decode.loss_mask: 0.3384  decode.loss_dice: 0.6445  decode.d0.loss_cls: 0.6815  decode.d0.loss_mask: 0.3389  decode.d0.loss_dice: 0.6498  decode.d1.loss_cls: 0.0275  decode.d1.loss_mask: 0.3380  decode.d1.loss_dice: 0.6416  decode.d2.loss_cls: 0.0281  decode.d2.loss_mask: 0.3423  decode.d2.loss_dice: 0.6366  decode.d3.loss_cls: 0.0260  decode.d3.loss_mask: 0.3392  decode.d3.loss_dice: 0.6285  decode.d4.loss_cls: 0.0239  decode.d4.loss_mask: 0.3424  decode.d4.loss_dice: 0.6591  decode.d5.loss_cls: 0.0487  decode.d5.loss_mask: 0.3407  decode.d5.loss_dice: 0.6608  decode.d6.loss_cls: 0.0303  decode.d6.loss_mask: 0.3366  decode.d6.loss_dice: 0.6450  decode.d7.loss_cls: 0.0481  decode.d7.loss_mask: 0.3371  decode.d7.loss_dice: 0.6474  decode.d8.loss_cls: 0.0191  decode.d8.loss_mask: 0.3431  decode.d8.loss_dice: 0.6245
2023/06/15 21:29:00 - mmengine - INFO - Iter(train) [1410/3000]  lr: 9.9207e-06  eta: 0:12:18  time: 0.4559  data_time: 0.0041  memory: 8331  grad_norm: 297.7492  loss: 11.0862  decode.loss_cls: 0.0571  decode.loss_mask: 0.3743  decode.loss_dice: 0.6401  decode.d0.loss_cls: 0.6311  decode.d0.loss_mask: 0.3651  decode.d0.loss_dice: 0.6246  decode.d1.loss_cls: 0.0383  decode.d1.loss_mask: 0.3672  decode.d1.loss_dice: 0.6046  decode.d2.loss_cls: 0.0279  decode.d2.loss_mask: 0.3696  decode.d2.loss_dice: 0.6202  decode.d3.loss_cls: 0.0439  decode.d3.loss_mask: 0.3693  decode.d3.loss_dice: 0.6336  decode.d4.loss_cls: 0.0421  decode.d4.loss_mask: 0.3705  decode.d4.loss_dice: 0.6306  decode.d5.loss_cls: 0.0474  decode.d5.loss_mask: 0.3721  decode.d5.loss_dice: 0.6345  decode.d6.loss_cls: 0.0576  decode.d6.loss_mask: 0.3766  decode.d6.loss_dice: 0.6421  decode.d7.loss_cls: 0.0597  decode.d7.loss_mask: 0.3762  decode.d7.loss_dice: 0.6326  decode.d8.loss_cls: 0.0683  decode.d8.loss_mask: 0.3751  decode.d8.loss_dice: 0.6339
2023/06/15 21:29:14 - mmengine - INFO - Iter(train) [1440/3000]  lr: 9.9190e-06  eta: 0:12:04  time: 0.4573  data_time: 0.0043  memory: 8331  grad_norm: 167.1512  loss: 10.6649  decode.loss_cls: 0.0272  decode.loss_mask: 0.3862  decode.loss_dice: 0.5984  decode.d0.loss_cls: 0.5667  decode.d0.loss_mask: 0.3856  decode.d0.loss_dice: 0.6003  decode.d1.loss_cls: 0.0390  decode.d1.loss_mask: 0.3920  decode.d1.loss_dice: 0.5859  decode.d2.loss_cls: 0.0318  decode.d2.loss_mask: 0.3943  decode.d2.loss_dice: 0.6015  decode.d3.loss_cls: 0.0177  decode.d3.loss_mask: 0.3942  decode.d3.loss_dice: 0.5910  decode.d4.loss_cls: 0.0193  decode.d4.loss_mask: 0.3953  decode.d4.loss_dice: 0.5831  decode.d5.loss_cls: 0.0221  decode.d5.loss_mask: 0.3914  decode.d5.loss_dice: 0.5887  decode.d6.loss_cls: 0.0448  decode.d6.loss_mask: 0.3824  decode.d6.loss_dice: 0.5802  decode.d7.loss_cls: 0.0503  decode.d7.loss_mask: 0.3925  decode.d7.loss_dice: 0.5837  decode.d8.loss_cls: 0.0476  decode.d8.loss_mask: 0.3856  decode.d8.loss_dice: 0.5859
2023/06/15 21:29:28 - mmengine - INFO - Iter(train) [1470/3000]  lr: 9.9173e-06  eta: 0:11:50  time: 0.4556  data_time: 0.0045  memory: 8331  grad_norm: 379.0072  loss: 12.7172  decode.loss_cls: 0.0430  decode.loss_mask: 0.4794  decode.loss_dice: 0.6833  decode.d0.loss_cls: 0.5102  decode.d0.loss_mask: 0.4734  decode.d0.loss_dice: 0.6822  decode.d1.loss_cls: 0.0422  decode.d1.loss_mask: 0.4795  decode.d1.loss_dice: 0.6943  decode.d2.loss_cls: 0.0418  decode.d2.loss_mask: 0.4791  decode.d2.loss_dice: 0.7006  decode.d3.loss_cls: 0.0504  decode.d3.loss_mask: 0.4767  decode.d3.loss_dice: 0.6898  decode.d4.loss_cls: 0.0512  decode.d4.loss_mask: 0.4783  decode.d4.loss_dice: 0.6927  decode.d5.loss_cls: 0.0515  decode.d5.loss_mask: 0.4845  decode.d5.loss_dice: 0.6949  decode.d6.loss_cls: 0.0554  decode.d6.loss_mask: 0.4819  decode.d6.loss_dice: 0.6882  decode.d7.loss_cls: 0.0447  decode.d7.loss_mask: 0.4800  decode.d7.loss_dice: 0.6903  decode.d8.loss_cls: 0.0955  decode.d8.loss_mask: 0.4832  decode.d8.loss_dice: 0.7190
2023/06/15 21:29:41 - mmengine - INFO - Iter(train) [1500/3000]  lr: 9.9156e-06  eta: 0:11:36  time: 0.4558  data_time: 0.0042  memory: 8331  grad_norm: 181.0816  loss: 9.2515  decode.loss_cls: 0.0082  decode.loss_mask: 0.3962  decode.loss_dice: 0.4769  decode.d0.loss_cls: 0.4837  decode.d0.loss_mask: 0.3981  decode.d0.loss_dice: 0.4602  decode.d1.loss_cls: 0.0171  decode.d1.loss_mask: 0.3970  decode.d1.loss_dice: 0.4750  decode.d2.loss_cls: 0.0069  decode.d2.loss_mask: 0.3941  decode.d2.loss_dice: 0.4745  decode.d3.loss_cls: 0.0063  decode.d3.loss_mask: 0.3958  decode.d3.loss_dice: 0.4853  decode.d4.loss_cls: 0.0067  decode.d4.loss_mask: 0.3928  decode.d4.loss_dice: 0.4682  decode.d5.loss_cls: 0.0077  decode.d5.loss_mask: 0.3981  decode.d5.loss_dice: 0.4686  decode.d6.loss_cls: 0.0089  decode.d6.loss_mask: 0.4001  decode.d6.loss_dice: 0.4745  decode.d7.loss_cls: 0.0076  decode.d7.loss_mask: 0.3974  decode.d7.loss_dice: 0.4722  decode.d8.loss_cls: 0.0070  decode.d8.loss_mask: 0.3939  decode.d8.loss_dice: 0.4723
2023/06/15 21:29:41 - mmengine - INFO - Saving checkpoint at 1500 iterations
2023/06/15 21:29:48 - mmengine - INFO - per class results:
2023/06/15 21:29:48 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 98.12 | 99.08 |
|    watermelon_green   | 95.75 | 98.12 |
|    watermelon_white   | 93.62 | 95.86 |
| watermelon_seed_black | 84.03 | 91.75 |
| watermelon_seed_white | 69.16 | 77.17 |
|       Unlabeled       | 54.45 | 56.49 |
+-----------------------+-------+-------+
2023/06/15 21:29:48 - mmengine - INFO - Iter(val) [11/11]    aAcc: 97.9000  mIoU: 82.5200  mAcc: 86.4100  data_time: 0.0042  time: 0.1112
2023/06/15 21:29:48 - mmengine - INFO - The previous best checkpoint D:\ACpycharm\myProjeycs\mmsegmentation-main\tools\work_dirs\AwatermelonMask2Former\best_mIoU_iter_900.pth is removed
2023/06/15 21:29:50 - mmengine - INFO - The best checkpoint with 82.5200 mIoU at 1500 iter is saved to best_mIoU_iter_1500.pth.
2023/06/15 21:30:04 - mmengine - INFO - Iter(train) [1530/3000]  lr: 9.9140e-06  eta: 0:11:23  time: 0.4551  data_time: 0.0041  memory: 8331  grad_norm: 121.7795  loss: 9.1396  decode.loss_cls: 0.0144  decode.loss_mask: 0.3194  decode.loss_dice: 0.5301  decode.d0.loss_cls: 0.4333  decode.d0.loss_mask: 0.3242  decode.d0.loss_dice: 0.5418  decode.d1.loss_cls: 0.0313  decode.d1.loss_mask: 0.3207  decode.d1.loss_dice: 0.5158  decode.d2.loss_cls: 0.0103  decode.d2.loss_mask: 0.3214  decode.d2.loss_dice: 0.5197  decode.d3.loss_cls: 0.0122  decode.d3.loss_mask: 0.3202  decode.d3.loss_dice: 0.5257  decode.d4.loss_cls: 0.0585  decode.d4.loss_mask: 0.3202  decode.d4.loss_dice: 0.5392  decode.d5.loss_cls: 0.0146  decode.d5.loss_mask: 0.3182  decode.d5.loss_dice: 0.5254  decode.d6.loss_cls: 0.0147  decode.d6.loss_mask: 0.3150  decode.d6.loss_dice: 0.5186  decode.d7.loss_cls: 0.0772  decode.d7.loss_mask: 0.3162  decode.d7.loss_dice: 0.5212  decode.d8.loss_cls: 0.0115  decode.d8.loss_mask: 0.3142  decode.d8.loss_dice: 0.5344
2023/06/15 21:30:17 - mmengine - INFO - Iter(train) [1560/3000]  lr: 9.9123e-06  eta: 0:11:09  time: 0.4560  data_time: 0.0044  memory: 8331  grad_norm: 183.8965  loss: 9.8829  decode.loss_cls: 0.0116  decode.loss_mask: 0.3845  decode.loss_dice: 0.5439  decode.d0.loss_cls: 0.4124  decode.d0.loss_mask: 0.3892  decode.d0.loss_dice: 0.5406  decode.d1.loss_cls: 0.0511  decode.d1.loss_mask: 0.3857  decode.d1.loss_dice: 0.5457  decode.d2.loss_cls: 0.0218  decode.d2.loss_mask: 0.3880  decode.d2.loss_dice: 0.5658  decode.d3.loss_cls: 0.0139  decode.d3.loss_mask: 0.3812  decode.d3.loss_dice: 0.5437  decode.d4.loss_cls: 0.0149  decode.d4.loss_mask: 0.3860  decode.d4.loss_dice: 0.5481  decode.d5.loss_cls: 0.0147  decode.d5.loss_mask: 0.3849  decode.d5.loss_dice: 0.5469  decode.d6.loss_cls: 0.0123  decode.d6.loss_mask: 0.3851  decode.d6.loss_dice: 0.5413  decode.d7.loss_cls: 0.0149  decode.d7.loss_mask: 0.3878  decode.d7.loss_dice: 0.5257  decode.d8.loss_cls: 0.0110  decode.d8.loss_mask: 0.3854  decode.d8.loss_dice: 0.5448
2023/06/15 21:30:31 - mmengine - INFO - Iter(train) [1590/3000]  lr: 9.9106e-06  eta: 0:10:55  time: 0.4553  data_time: 0.0040  memory: 8331  grad_norm: 230.5985  loss: 10.1581  decode.loss_cls: 0.0660  decode.loss_mask: 0.3499  decode.loss_dice: 0.5793  decode.d0.loss_cls: 0.4095  decode.d0.loss_mask: 0.3538  decode.d0.loss_dice: 0.5844  decode.d1.loss_cls: 0.0484  decode.d1.loss_mask: 0.3541  decode.d1.loss_dice: 0.5874  decode.d2.loss_cls: 0.0220  decode.d2.loss_mask: 0.3543  decode.d2.loss_dice: 0.5852  decode.d3.loss_cls: 0.0192  decode.d3.loss_mask: 0.3558  decode.d3.loss_dice: 0.5848  decode.d4.loss_cls: 0.0263  decode.d4.loss_mask: 0.3513  decode.d4.loss_dice: 0.5739  decode.d5.loss_cls: 0.0855  decode.d5.loss_mask: 0.3534  decode.d5.loss_dice: 0.5703  decode.d6.loss_cls: 0.0403  decode.d6.loss_mask: 0.3520  decode.d6.loss_dice: 0.5774  decode.d7.loss_cls: 0.0386  decode.d7.loss_mask: 0.3546  decode.d7.loss_dice: 0.5945  decode.d8.loss_cls: 0.0560  decode.d8.loss_mask: 0.3488  decode.d8.loss_dice: 0.5812
2023/06/15 21:30:45 - mmengine - INFO - Iter(train) [1620/3000]  lr: 9.9089e-06  eta: 0:10:41  time: 0.4553  data_time: 0.0042  memory: 8331  grad_norm: 207.7540  loss: 11.1261  decode.loss_cls: 0.0144  decode.loss_mask: 0.3903  decode.loss_dice: 0.6852  decode.d0.loss_cls: 0.3261  decode.d0.loss_mask: 0.3854  decode.d0.loss_dice: 0.6945  decode.d1.loss_cls: 0.0184  decode.d1.loss_mask: 0.3858  decode.d1.loss_dice: 0.6747  decode.d2.loss_cls: 0.0101  decode.d2.loss_mask: 0.3856  decode.d2.loss_dice: 0.6798  decode.d3.loss_cls: 0.0069  decode.d3.loss_mask: 0.3824  decode.d3.loss_dice: 0.6791  decode.d4.loss_cls: 0.0112  decode.d4.loss_mask: 0.3935  decode.d4.loss_dice: 0.6931  decode.d5.loss_cls: 0.0154  decode.d5.loss_mask: 0.3868  decode.d5.loss_dice: 0.6839  decode.d6.loss_cls: 0.0109  decode.d6.loss_mask: 0.3850  decode.d6.loss_dice: 0.6752  decode.d7.loss_cls: 0.0112  decode.d7.loss_mask: 0.3843  decode.d7.loss_dice: 0.6782  decode.d8.loss_cls: 0.0110  decode.d8.loss_mask: 0.3899  decode.d8.loss_dice: 0.6777
2023/06/15 21:30:58 - mmengine - INFO - Iter(train) [1650/3000]  lr: 9.9072e-06  eta: 0:10:27  time: 0.4561  data_time: 0.0041  memory: 8331  grad_norm: 159.9281  loss: 10.6205  decode.loss_cls: 0.0724  decode.loss_mask: 0.3735  decode.loss_dice: 0.5889  decode.d0.loss_cls: 0.3163  decode.d0.loss_mask: 0.3707  decode.d0.loss_dice: 0.5986  decode.d1.loss_cls: 0.0917  decode.d1.loss_mask: 0.3716  decode.d1.loss_dice: 0.5986  decode.d2.loss_cls: 0.0992  decode.d2.loss_mask: 0.3650  decode.d2.loss_dice: 0.5918  decode.d3.loss_cls: 0.0205  decode.d3.loss_mask: 0.3715  decode.d3.loss_dice: 0.5894  decode.d4.loss_cls: 0.0909  decode.d4.loss_mask: 0.3752  decode.d4.loss_dice: 0.6004  decode.d5.loss_cls: 0.0830  decode.d5.loss_mask: 0.3760  decode.d5.loss_dice: 0.6071  decode.d6.loss_cls: 0.0152  decode.d6.loss_mask: 0.3737  decode.d6.loss_dice: 0.6021  decode.d7.loss_cls: 0.0849  decode.d7.loss_mask: 0.3736  decode.d7.loss_dice: 0.5954  decode.d8.loss_cls: 0.0654  decode.d8.loss_mask: 0.3703  decode.d8.loss_dice: 0.5876
2023/06/15 21:31:12 - mmengine - INFO - Iter(train) [1680/3000]  lr: 9.9055e-06  eta: 0:10:13  time: 0.4562  data_time: 0.0044  memory: 8331  grad_norm: 131.4194  loss: 9.9499  decode.loss_cls: 0.0049  decode.loss_mask: 0.4209  decode.loss_dice: 0.5266  decode.d0.loss_cls: 0.3259  decode.d0.loss_mask: 0.4230  decode.d0.loss_dice: 0.5229  decode.d1.loss_cls: 0.1120  decode.d1.loss_mask: 0.4068  decode.d1.loss_dice: 0.5263  decode.d2.loss_cls: 0.0247  decode.d2.loss_mask: 0.4171  decode.d2.loss_dice: 0.5076  decode.d3.loss_cls: 0.0037  decode.d3.loss_mask: 0.4189  decode.d3.loss_dice: 0.5207  decode.d4.loss_cls: 0.0136  decode.d4.loss_mask: 0.4176  decode.d4.loss_dice: 0.5084  decode.d5.loss_cls: 0.0148  decode.d5.loss_mask: 0.4260  decode.d5.loss_dice: 0.5211  decode.d6.loss_cls: 0.0032  decode.d6.loss_mask: 0.4254  decode.d6.loss_dice: 0.5297  decode.d7.loss_cls: 0.0116  decode.d7.loss_mask: 0.4261  decode.d7.loss_dice: 0.5259  decode.d8.loss_cls: 0.0078  decode.d8.loss_mask: 0.4260  decode.d8.loss_dice: 0.5308
2023/06/15 21:31:26 - mmengine - INFO - Iter(train) [1710/3000]  lr: 9.9038e-06  eta: 0:09:58  time: 0.4534  data_time: 0.0040  memory: 8331  grad_norm: 243.0073  loss: 10.2687  decode.loss_cls: 0.0209  decode.loss_mask: 0.3806  decode.loss_dice: 0.6094  decode.d0.loss_cls: 0.2840  decode.d0.loss_mask: 0.3750  decode.d0.loss_dice: 0.5805  decode.d1.loss_cls: 0.0279  decode.d1.loss_mask: 0.3807  decode.d1.loss_dice: 0.5883  decode.d2.loss_cls: 0.0198  decode.d2.loss_mask: 0.3824  decode.d2.loss_dice: 0.5970  decode.d3.loss_cls: 0.0197  decode.d3.loss_mask: 0.3792  decode.d3.loss_dice: 0.5989  decode.d4.loss_cls: 0.0218  decode.d4.loss_mask: 0.3839  decode.d4.loss_dice: 0.5970  decode.d5.loss_cls: 0.0204  decode.d5.loss_mask: 0.3845  decode.d5.loss_dice: 0.6006  decode.d6.loss_cls: 0.0228  decode.d6.loss_mask: 0.3784  decode.d6.loss_dice: 0.5880  decode.d7.loss_cls: 0.0217  decode.d7.loss_mask: 0.3806  decode.d7.loss_dice: 0.6041  decode.d8.loss_cls: 0.0222  decode.d8.loss_mask: 0.3854  decode.d8.loss_dice: 0.6127
2023/06/15 21:31:39 - mmengine - INFO - Iter(train) [1740/3000]  lr: 9.9021e-06  eta: 0:09:44  time: 0.4560  data_time: 0.0040  memory: 8331  grad_norm: 141.6867  loss: 10.3861  decode.loss_cls: 0.0282  decode.loss_mask: 0.3644  decode.loss_dice: 0.6382  decode.d0.loss_cls: 0.2696  decode.d0.loss_mask: 0.3680  decode.d0.loss_dice: 0.6128  decode.d1.loss_cls: 0.0382  decode.d1.loss_mask: 0.3621  decode.d1.loss_dice: 0.6167  decode.d2.loss_cls: 0.0317  decode.d2.loss_mask: 0.3557  decode.d2.loss_dice: 0.6109  decode.d3.loss_cls: 0.0298  decode.d3.loss_mask: 0.3642  decode.d3.loss_dice: 0.6127  decode.d4.loss_cls: 0.0259  decode.d4.loss_mask: 0.3642  decode.d4.loss_dice: 0.6284  decode.d5.loss_cls: 0.0303  decode.d5.loss_mask: 0.3665  decode.d5.loss_dice: 0.6240  decode.d6.loss_cls: 0.0322  decode.d6.loss_mask: 0.3658  decode.d6.loss_dice: 0.6023  decode.d7.loss_cls: 0.0257  decode.d7.loss_mask: 0.3653  decode.d7.loss_dice: 0.6336  decode.d8.loss_cls: 0.0324  decode.d8.loss_mask: 0.3659  decode.d8.loss_dice: 0.6205
2023/06/15 21:31:53 - mmengine - INFO - Iter(train) [1770/3000]  lr: 9.9004e-06  eta: 0:09:30  time: 0.4549  data_time: 0.0040  memory: 8331  grad_norm: 119.4580  loss: 9.1086  decode.loss_cls: 0.0065  decode.loss_mask: 0.3649  decode.loss_dice: 0.5126  decode.d0.loss_cls: 0.2590  decode.d0.loss_mask: 0.3663  decode.d0.loss_dice: 0.5038  decode.d1.loss_cls: 0.0334  decode.d1.loss_mask: 0.3636  decode.d1.loss_dice: 0.4993  decode.d2.loss_cls: 0.0076  decode.d2.loss_mask: 0.3667  decode.d2.loss_dice: 0.5069  decode.d3.loss_cls: 0.0047  decode.d3.loss_mask: 0.3654  decode.d3.loss_dice: 0.5149  decode.d4.loss_cls: 0.0060  decode.d4.loss_mask: 0.3689  decode.d4.loss_dice: 0.5135  decode.d5.loss_cls: 0.0065  decode.d5.loss_mask: 0.3643  decode.d5.loss_dice: 0.5128  decode.d6.loss_cls: 0.0053  decode.d6.loss_mask: 0.3682  decode.d6.loss_dice: 0.5215  decode.d7.loss_cls: 0.0052  decode.d7.loss_mask: 0.3652  decode.d7.loss_dice: 0.5155  decode.d8.loss_cls: 0.0059  decode.d8.loss_mask: 0.3686  decode.d8.loss_dice: 0.5058
2023/06/15 21:32:07 - mmengine - INFO - Iter(train) [1800/3000]  lr: 9.8987e-06  eta: 0:09:16  time: 0.4543  data_time: 0.0041  memory: 8331  grad_norm: 168.1016  loss: 8.9279  decode.loss_cls: 0.0168  decode.loss_mask: 0.2944  decode.loss_dice: 0.5651  decode.d0.loss_cls: 0.2227  decode.d0.loss_mask: 0.2948  decode.d0.loss_dice: 0.5659  decode.d1.loss_cls: 0.0207  decode.d1.loss_mask: 0.2936  decode.d1.loss_dice: 0.5581  decode.d2.loss_cls: 0.0234  decode.d2.loss_mask: 0.2938  decode.d2.loss_dice: 0.5467  decode.d3.loss_cls: 0.0143  decode.d3.loss_mask: 0.2953  decode.d3.loss_dice: 0.5782  decode.d4.loss_cls: 0.0190  decode.d4.loss_mask: 0.2942  decode.d4.loss_dice: 0.5653  decode.d5.loss_cls: 0.0193  decode.d5.loss_mask: 0.2944  decode.d5.loss_dice: 0.5565  decode.d6.loss_cls: 0.0129  decode.d6.loss_mask: 0.2947  decode.d6.loss_dice: 0.5550  decode.d7.loss_cls: 0.0147  decode.d7.loss_mask: 0.2946  decode.d7.loss_dice: 0.5607  decode.d8.loss_cls: 0.0165  decode.d8.loss_mask: 0.2949  decode.d8.loss_dice: 0.5514
2023/06/15 21:32:07 - mmengine - INFO - Saving checkpoint at 1800 iterations
2023/06/15 21:32:13 - mmengine - INFO - per class results:
2023/06/15 21:32:13 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 98.14 |  98.9 |
|    watermelon_green   | 95.68 | 98.57 |
|    watermelon_white   | 94.05 | 96.85 |
| watermelon_seed_black | 85.03 | 90.28 |
| watermelon_seed_white | 70.96 | 87.93 |
|       Unlabeled       | 58.28 | 65.98 |
+-----------------------+-------+-------+
2023/06/15 21:32:13 - mmengine - INFO - Iter(val) [11/11]    aAcc: 97.9600  mIoU: 83.6900  mAcc: 89.7500  data_time: 0.0037  time: 0.1112
2023/06/15 21:32:13 - mmengine - INFO - The previous best checkpoint D:\ACpycharm\myProjeycs\mmsegmentation-main\tools\work_dirs\AwatermelonMask2Former\best_mIoU_iter_1500.pth is removed
2023/06/15 21:32:15 - mmengine - INFO - The best checkpoint with 83.6900 mIoU at 1800 iter is saved to best_mIoU_iter_1800.pth.
2023/06/15 21:32:29 - mmengine - INFO - Iter(train) [1830/3000]  lr: 9.8971e-06  eta: 0:09:04  time: 0.4541  data_time: 0.0041  memory: 8331  grad_norm: 115.8697  loss: 8.7073  decode.loss_cls: 0.0117  decode.loss_mask: 0.3224  decode.loss_dice: 0.5128  decode.d0.loss_cls: 0.2183  decode.d0.loss_mask: 0.3204  decode.d0.loss_dice: 0.5071  decode.d1.loss_cls: 0.0185  decode.d1.loss_mask: 0.3211  decode.d1.loss_dice: 0.5215  decode.d2.loss_cls: 0.0130  decode.d2.loss_mask: 0.3215  decode.d2.loss_dice: 0.5202  decode.d3.loss_cls: 0.0096  decode.d3.loss_mask: 0.3231  decode.d3.loss_dice: 0.5154  decode.d4.loss_cls: 0.0106  decode.d4.loss_mask: 0.3185  decode.d4.loss_dice: 0.5073  decode.d5.loss_cls: 0.0111  decode.d5.loss_mask: 0.3238  decode.d5.loss_dice: 0.5153  decode.d6.loss_cls: 0.0111  decode.d6.loss_mask: 0.3257  decode.d6.loss_dice: 0.5145  decode.d7.loss_cls: 0.0111  decode.d7.loss_mask: 0.3247  decode.d7.loss_dice: 0.5196  decode.d8.loss_cls: 0.0128  decode.d8.loss_mask: 0.3230  decode.d8.loss_dice: 0.5217
2023/06/15 21:32:42 - mmengine - INFO - Iter(train) [1860/3000]  lr: 9.8954e-06  eta: 0:08:49  time: 0.4571  data_time: 0.0042  memory: 8331  grad_norm: 205.6520  loss: 8.9041  decode.loss_cls: 0.0112  decode.loss_mask: 0.3333  decode.loss_dice: 0.5266  decode.d0.loss_cls: 0.1955  decode.d0.loss_mask: 0.3364  decode.d0.loss_dice: 0.5389  decode.d1.loss_cls: 0.0242  decode.d1.loss_mask: 0.3354  decode.d1.loss_dice: 0.5465  decode.d2.loss_cls: 0.0095  decode.d2.loss_mask: 0.3357  decode.d2.loss_dice: 0.5288  decode.d3.loss_cls: 0.0080  decode.d3.loss_mask: 0.3339  decode.d3.loss_dice: 0.5004  decode.d4.loss_cls: 0.0089  decode.d4.loss_mask: 0.3325  decode.d4.loss_dice: 0.5199  decode.d5.loss_cls: 0.0092  decode.d5.loss_mask: 0.3361  decode.d5.loss_dice: 0.5246  decode.d6.loss_cls: 0.0090  decode.d6.loss_mask: 0.3303  decode.d6.loss_dice: 0.5257  decode.d7.loss_cls: 0.0102  decode.d7.loss_mask: 0.3378  decode.d7.loss_dice: 0.5316  decode.d8.loss_cls: 0.0089  decode.d8.loss_mask: 0.3306  decode.d8.loss_dice: 0.5247
2023/06/15 21:32:56 - mmengine - INFO - Iter(train) [1890/3000]  lr: 9.8937e-06  eta: 0:08:35  time: 0.4557  data_time: 0.0043  memory: 8331  grad_norm: 188.3115  loss: 10.1306  decode.loss_cls: 0.0499  decode.loss_mask: 0.3359  decode.loss_dice: 0.6125  decode.d0.loss_cls: 0.1868  decode.d0.loss_mask: 0.3352  decode.d0.loss_dice: 0.6274  decode.d1.loss_cls: 0.0452  decode.d1.loss_mask: 0.3299  decode.d1.loss_dice: 0.6223  decode.d2.loss_cls: 0.0350  decode.d2.loss_mask: 0.3339  decode.d2.loss_dice: 0.6030  decode.d3.loss_cls: 0.0513  decode.d3.loss_mask: 0.3322  decode.d3.loss_dice: 0.6156  decode.d4.loss_cls: 0.0505  decode.d4.loss_mask: 0.3343  decode.d4.loss_dice: 0.6186  decode.d5.loss_cls: 0.0470  decode.d5.loss_mask: 0.3350  decode.d5.loss_dice: 0.6191  decode.d6.loss_cls: 0.0475  decode.d6.loss_mask: 0.3341  decode.d6.loss_dice: 0.6134  decode.d7.loss_cls: 0.0528  decode.d7.loss_mask: 0.3329  decode.d7.loss_dice: 0.6237  decode.d8.loss_cls: 0.0461  decode.d8.loss_mask: 0.3379  decode.d8.loss_dice: 0.6217
2023/06/15 21:33:10 - mmengine - INFO - Iter(train) [1920/3000]  lr: 9.8920e-06  eta: 0:08:21  time: 0.4548  data_time: 0.0041  memory: 8331  grad_norm: 210.7266  loss: 9.2709  decode.loss_cls: 0.0075  decode.loss_mask: 0.3608  decode.loss_dice: 0.5486  decode.d0.loss_cls: 0.1920  decode.d0.loss_mask: 0.3687  decode.d0.loss_dice: 0.5488  decode.d1.loss_cls: 0.0258  decode.d1.loss_mask: 0.3632  decode.d1.loss_dice: 0.5218  decode.d2.loss_cls: 0.0076  decode.d2.loss_mask: 0.3598  decode.d2.loss_dice: 0.5264  decode.d3.loss_cls: 0.0080  decode.d3.loss_mask: 0.3643  decode.d3.loss_dice: 0.5315  decode.d4.loss_cls: 0.0059  decode.d4.loss_mask: 0.3601  decode.d4.loss_dice: 0.5278  decode.d5.loss_cls: 0.0063  decode.d5.loss_mask: 0.3624  decode.d5.loss_dice: 0.5288  decode.d6.loss_cls: 0.0071  decode.d6.loss_mask: 0.3688  decode.d6.loss_dice: 0.5491  decode.d7.loss_cls: 0.0060  decode.d7.loss_mask: 0.3642  decode.d7.loss_dice: 0.5417  decode.d8.loss_cls: 0.0068  decode.d8.loss_mask: 0.3625  decode.d8.loss_dice: 0.5386
2023/06/15 21:33:23 - mmengine - INFO - Iter(train) [1950/3000]  lr: 9.8903e-06  eta: 0:08:07  time: 0.4566  data_time: 0.0041  memory: 8331  grad_norm: 113.2770  loss: 8.3820  decode.loss_cls: 0.0152  decode.loss_mask: 0.3301  decode.loss_dice: 0.4819  decode.d0.loss_cls: 0.1872  decode.d0.loss_mask: 0.3234  decode.d0.loss_dice: 0.4837  decode.d1.loss_cls: 0.0349  decode.d1.loss_mask: 0.3242  decode.d1.loss_dice: 0.4712  decode.d2.loss_cls: 0.0108  decode.d2.loss_mask: 0.3318  decode.d2.loss_dice: 0.4880  decode.d3.loss_cls: 0.0116  decode.d3.loss_mask: 0.3282  decode.d3.loss_dice: 0.4675  decode.d4.loss_cls: 0.0190  decode.d4.loss_mask: 0.3297  decode.d4.loss_dice: 0.4731  decode.d5.loss_cls: 0.0115  decode.d5.loss_mask: 0.3260  decode.d5.loss_dice: 0.4865  decode.d6.loss_cls: 0.0177  decode.d6.loss_mask: 0.3252  decode.d6.loss_dice: 0.4686  decode.d7.loss_cls: 0.0156  decode.d7.loss_mask: 0.3212  decode.d7.loss_dice: 0.4823  decode.d8.loss_cls: 0.0110  decode.d8.loss_mask: 0.3272  decode.d8.loss_dice: 0.4778
2023/06/15 21:33:37 - mmengine - INFO - Iter(train) [1980/3000]  lr: 9.8886e-06  eta: 0:07:53  time: 0.4557  data_time: 0.0042  memory: 8331  grad_norm: 197.0963  loss: 10.1393  decode.loss_cls: 0.0148  decode.loss_mask: 0.3617  decode.loss_dice: 0.6178  decode.d0.loss_cls: 0.1608  decode.d0.loss_mask: 0.3645  decode.d0.loss_dice: 0.6064  decode.d1.loss_cls: 0.0476  decode.d1.loss_mask: 0.3629  decode.d1.loss_dice: 0.6126  decode.d2.loss_cls: 0.0249  decode.d2.loss_mask: 0.3598  decode.d2.loss_dice: 0.6116  decode.d3.loss_cls: 0.0210  decode.d3.loss_mask: 0.3610  decode.d3.loss_dice: 0.6250  decode.d4.loss_cls: 0.0215  decode.d4.loss_mask: 0.3611  decode.d4.loss_dice: 0.6239  decode.d5.loss_cls: 0.0198  decode.d5.loss_mask: 0.3603  decode.d5.loss_dice: 0.6331  decode.d6.loss_cls: 0.0128  decode.d6.loss_mask: 0.3635  decode.d6.loss_dice: 0.6146  decode.d7.loss_cls: 0.0069  decode.d7.loss_mask: 0.3622  decode.d7.loss_dice: 0.6167  decode.d8.loss_cls: 0.0116  decode.d8.loss_mask: 0.3636  decode.d8.loss_dice: 0.6155
2023/06/15 21:33:46 - mmengine - INFO - Exp name: AwatermelonMask2Former_20230615_211716
2023/06/15 21:33:51 - mmengine - INFO - Iter(train) [2010/3000]  lr: 9.8869e-06  eta: 0:07:39  time: 0.4564  data_time: 0.0041  memory: 8331  grad_norm: 174.9825  loss: 9.1821  decode.loss_cls: 0.0194  decode.loss_mask: 0.4212  decode.loss_dice: 0.4809  decode.d0.loss_cls: 0.1601  decode.d0.loss_mask: 0.4218  decode.d0.loss_dice: 0.4708  decode.d1.loss_cls: 0.0307  decode.d1.loss_mask: 0.4165  decode.d1.loss_dice: 0.4772  decode.d2.loss_cls: 0.0122  decode.d2.loss_mask: 0.4223  decode.d2.loss_dice: 0.4711  decode.d3.loss_cls: 0.0102  decode.d3.loss_mask: 0.4188  decode.d3.loss_dice: 0.4738  decode.d4.loss_cls: 0.0072  decode.d4.loss_mask: 0.4176  decode.d4.loss_dice: 0.4731  decode.d5.loss_cls: 0.0067  decode.d5.loss_mask: 0.4188  decode.d5.loss_dice: 0.4643  decode.d6.loss_cls: 0.0065  decode.d6.loss_mask: 0.4182  decode.d6.loss_dice: 0.4748  decode.d7.loss_cls: 0.0052  decode.d7.loss_mask: 0.4156  decode.d7.loss_dice: 0.4712  decode.d8.loss_cls: 0.0072  decode.d8.loss_mask: 0.4223  decode.d8.loss_dice: 0.4665
2023/06/15 21:34:05 - mmengine - INFO - Iter(train) [2040/3000]  lr: 9.8852e-06  eta: 0:07:25  time: 0.4550  data_time: 0.0041  memory: 8320  grad_norm: 300.2177  loss: 9.2251  decode.loss_cls: 0.0103  decode.loss_mask: 0.3809  decode.loss_dice: 0.5200  decode.d0.loss_cls: 0.1503  decode.d0.loss_mask: 0.3843  decode.d0.loss_dice: 0.5197  decode.d1.loss_cls: 0.0173  decode.d1.loss_mask: 0.3775  decode.d1.loss_dice: 0.5104  decode.d2.loss_cls: 0.0062  decode.d2.loss_mask: 0.3825  decode.d2.loss_dice: 0.5148  decode.d3.loss_cls: 0.0104  decode.d3.loss_mask: 0.3821  decode.d3.loss_dice: 0.5184  decode.d4.loss_cls: 0.0101  decode.d4.loss_mask: 0.3812  decode.d4.loss_dice: 0.5059  decode.d5.loss_cls: 0.0142  decode.d5.loss_mask: 0.3845  decode.d5.loss_dice: 0.5144  decode.d6.loss_cls: 0.0188  decode.d6.loss_mask: 0.3849  decode.d6.loss_dice: 0.5070  decode.d7.loss_cls: 0.0096  decode.d7.loss_mask: 0.3837  decode.d7.loss_dice: 0.5142  decode.d8.loss_cls: 0.0113  decode.d8.loss_mask: 0.3816  decode.d8.loss_dice: 0.5186
2023/06/15 21:34:18 - mmengine - INFO - Iter(train) [2070/3000]  lr: 9.8835e-06  eta: 0:07:11  time: 0.4557  data_time: 0.0042  memory: 8331  grad_norm: 78.5277  loss: 8.2650  decode.loss_cls: 0.0037  decode.loss_mask: 0.2995  decode.loss_dice: 0.5152  decode.d0.loss_cls: 0.1418  decode.d0.loss_mask: 0.2999  decode.d0.loss_dice: 0.5056  decode.d1.loss_cls: 0.0170  decode.d1.loss_mask: 0.3002  decode.d1.loss_dice: 0.5023  decode.d2.loss_cls: 0.0042  decode.d2.loss_mask: 0.3000  decode.d2.loss_dice: 0.5076  decode.d3.loss_cls: 0.0042  decode.d3.loss_mask: 0.3003  decode.d3.loss_dice: 0.5191  decode.d4.loss_cls: 0.0040  decode.d4.loss_mask: 0.2962  decode.d4.loss_dice: 0.4963  decode.d5.loss_cls: 0.0039  decode.d5.loss_mask: 0.3002  decode.d5.loss_dice: 0.5200  decode.d6.loss_cls: 0.0040  decode.d6.loss_mask: 0.2992  decode.d6.loss_dice: 0.5123  decode.d7.loss_cls: 0.0042  decode.d7.loss_mask: 0.2945  decode.d7.loss_dice: 0.4995  decode.d8.loss_cls: 0.0034  decode.d8.loss_mask: 0.3006  decode.d8.loss_dice: 0.5062
2023/06/15 21:34:32 - mmengine - INFO - Iter(train) [2100/3000]  lr: 9.8819e-06  eta: 0:06:57  time: 0.4551  data_time: 0.0041  memory: 8331  grad_norm: 124.9152  loss: 9.2567  decode.loss_cls: 0.0108  decode.loss_mask: 0.3658  decode.loss_dice: 0.5472  decode.d0.loss_cls: 0.1412  decode.d0.loss_mask: 0.3654  decode.d0.loss_dice: 0.5354  decode.d1.loss_cls: 0.0194  decode.d1.loss_mask: 0.3652  decode.d1.loss_dice: 0.5315  decode.d2.loss_cls: 0.0080  decode.d2.loss_mask: 0.3611  decode.d2.loss_dice: 0.5380  decode.d3.loss_cls: 0.0065  decode.d3.loss_mask: 0.3594  decode.d3.loss_dice: 0.5537  decode.d4.loss_cls: 0.0089  decode.d4.loss_mask: 0.3626  decode.d4.loss_dice: 0.5352  decode.d5.loss_cls: 0.0101  decode.d5.loss_mask: 0.3678  decode.d5.loss_dice: 0.5453  decode.d6.loss_cls: 0.0093  decode.d6.loss_mask: 0.3621  decode.d6.loss_dice: 0.5328  decode.d7.loss_cls: 0.0109  decode.d7.loss_mask: 0.3636  decode.d7.loss_dice: 0.5257  decode.d8.loss_cls: 0.0094  decode.d8.loss_mask: 0.3663  decode.d8.loss_dice: 0.5377
2023/06/15 21:34:32 - mmengine - INFO - Saving checkpoint at 2100 iterations
2023/06/15 21:34:38 - mmengine - INFO - per class results:
2023/06/15 21:34:38 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 98.14 | 98.68 |
|    watermelon_green   | 95.43 | 97.59 |
|    watermelon_white   | 93.63 | 97.61 |
| watermelon_seed_black | 83.65 | 93.19 |
| watermelon_seed_white | 72.28 |  90.2 |
|       Unlabeled       |  64.6 | 72.73 |
+-----------------------+-------+-------+
2023/06/15 21:34:38 - mmengine - INFO - Iter(val) [11/11]    aAcc: 97.8400  mIoU: 84.6200  mAcc: 91.6600  data_time: 0.0039  time: 0.1128
2023/06/15 21:34:38 - mmengine - INFO - The previous best checkpoint D:\ACpycharm\myProjeycs\mmsegmentation-main\tools\work_dirs\AwatermelonMask2Former\best_mIoU_iter_1800.pth is removed
2023/06/15 21:34:40 - mmengine - INFO - The best checkpoint with 84.6200 mIoU at 2100 iter is saved to best_mIoU_iter_2100.pth.
2023/06/15 21:34:54 - mmengine - INFO - Iter(train) [2130/3000]  lr: 9.8802e-06  eta: 0:06:44  time: 0.4572  data_time: 0.0040  memory: 8331  grad_norm: 184.3541  loss: 9.2231  decode.loss_cls: 0.0254  decode.loss_mask: 0.3715  decode.loss_dice: 0.5226  decode.d0.loss_cls: 0.1389  decode.d0.loss_mask: 0.3724  decode.d0.loss_dice: 0.5189  decode.d1.loss_cls: 0.0237  decode.d1.loss_mask: 0.3716  decode.d1.loss_dice: 0.5357  decode.d2.loss_cls: 0.0155  decode.d2.loss_mask: 0.3688  decode.d2.loss_dice: 0.5177  decode.d3.loss_cls: 0.0171  decode.d3.loss_mask: 0.3715  decode.d3.loss_dice: 0.5221  decode.d4.loss_cls: 0.0229  decode.d4.loss_mask: 0.3697  decode.d4.loss_dice: 0.5025  decode.d5.loss_cls: 0.0254  decode.d5.loss_mask: 0.3747  decode.d5.loss_dice: 0.4946  decode.d6.loss_cls: 0.0205  decode.d6.loss_mask: 0.3765  decode.d6.loss_dice: 0.5208  decode.d7.loss_cls: 0.0210  decode.d7.loss_mask: 0.3770  decode.d7.loss_dice: 0.5103  decode.d8.loss_cls: 0.0225  decode.d8.loss_mask: 0.3693  decode.d8.loss_dice: 0.5222
2023/06/15 21:35:08 - mmengine - INFO - Iter(train) [2160/3000]  lr: 9.8785e-06  eta: 0:06:30  time: 0.4562  data_time: 0.0044  memory: 8331  grad_norm: 102.7524  loss: 8.8305  decode.loss_cls: 0.0084  decode.loss_mask: 0.3218  decode.loss_dice: 0.5388  decode.d0.loss_cls: 0.1338  decode.d0.loss_mask: 0.3229  decode.d0.loss_dice: 0.5408  decode.d1.loss_cls: 0.0209  decode.d1.loss_mask: 0.3168  decode.d1.loss_dice: 0.5228  decode.d2.loss_cls: 0.0067  decode.d2.loss_mask: 0.3187  decode.d2.loss_dice: 0.5346  decode.d3.loss_cls: 0.0034  decode.d3.loss_mask: 0.3175  decode.d3.loss_dice: 0.5437  decode.d4.loss_cls: 0.0410  decode.d4.loss_mask: 0.3195  decode.d4.loss_dice: 0.5412  decode.d5.loss_cls: 0.0376  decode.d5.loss_mask: 0.3217  decode.d5.loss_dice: 0.5444  decode.d6.loss_cls: 0.0040  decode.d6.loss_mask: 0.3202  decode.d6.loss_dice: 0.5336  decode.d7.loss_cls: 0.0054  decode.d7.loss_mask: 0.3191  decode.d7.loss_dice: 0.5354  decode.d8.loss_cls: 0.0059  decode.d8.loss_mask: 0.3194  decode.d8.loss_dice: 0.5305
2023/06/15 21:35:21 - mmengine - INFO - Iter(train) [2190/3000]  lr: 9.8768e-06  eta: 0:06:16  time: 0.4555  data_time: 0.0042  memory: 8331  grad_norm: 110.4463  loss: 9.6702  decode.loss_cls: 0.0823  decode.loss_mask: 0.3652  decode.loss_dice: 0.5444  decode.d0.loss_cls: 0.1258  decode.d0.loss_mask: 0.3684  decode.d0.loss_dice: 0.5361  decode.d1.loss_cls: 0.0407  decode.d1.loss_mask: 0.3578  decode.d1.loss_dice: 0.5408  decode.d2.loss_cls: 0.0213  decode.d2.loss_mask: 0.3607  decode.d2.loss_dice: 0.5447  decode.d3.loss_cls: 0.0312  decode.d3.loss_mask: 0.3608  decode.d3.loss_dice: 0.5465  decode.d4.loss_cls: 0.0337  decode.d4.loss_mask: 0.3619  decode.d4.loss_dice: 0.5759  decode.d5.loss_cls: 0.0325  decode.d5.loss_mask: 0.3574  decode.d5.loss_dice: 0.5660  decode.d6.loss_cls: 0.0349  decode.d6.loss_mask: 0.3619  decode.d6.loss_dice: 0.5534  decode.d7.loss_cls: 0.0918  decode.d7.loss_mask: 0.3643  decode.d7.loss_dice: 0.5514  decode.d8.loss_cls: 0.0332  decode.d8.loss_mask: 0.3615  decode.d8.loss_dice: 0.5635
2023/06/15 21:35:35 - mmengine - INFO - Iter(train) [2220/3000]  lr: 9.8751e-06  eta: 0:06:02  time: 0.4634  data_time: 0.0041  memory: 8320  grad_norm: 246.9381  loss: 9.3893  decode.loss_cls: 0.0115  decode.loss_mask: 0.3575  decode.loss_dice: 0.5503  decode.d0.loss_cls: 0.1250  decode.d0.loss_mask: 0.3604  decode.d0.loss_dice: 0.5555  decode.d1.loss_cls: 0.0589  decode.d1.loss_mask: 0.3522  decode.d1.loss_dice: 0.5551  decode.d2.loss_cls: 0.0142  decode.d2.loss_mask: 0.3639  decode.d2.loss_dice: 0.5574  decode.d3.loss_cls: 0.0076  decode.d3.loss_mask: 0.3632  decode.d3.loss_dice: 0.5536  decode.d4.loss_cls: 0.0097  decode.d4.loss_mask: 0.3622  decode.d4.loss_dice: 0.5401  decode.d5.loss_cls: 0.0107  decode.d5.loss_mask: 0.3679  decode.d5.loss_dice: 0.5568  decode.d6.loss_cls: 0.0066  decode.d6.loss_mask: 0.3665  decode.d6.loss_dice: 0.5452  decode.d7.loss_cls: 0.0103  decode.d7.loss_mask: 0.3659  decode.d7.loss_dice: 0.5416  decode.d8.loss_cls: 0.0099  decode.d8.loss_mask: 0.3651  decode.d8.loss_dice: 0.5445
2023/06/15 21:35:49 - mmengine - INFO - Iter(train) [2250/3000]  lr: 9.8734e-06  eta: 0:05:48  time: 0.4555  data_time: 0.0041  memory: 8331  grad_norm: 127.7365  loss: 9.0082  decode.loss_cls: 0.0824  decode.loss_mask: 0.3277  decode.loss_dice: 0.5218  decode.d0.loss_cls: 0.1196  decode.d0.loss_mask: 0.3312  decode.d0.loss_dice: 0.5322  decode.d1.loss_cls: 0.0191  decode.d1.loss_mask: 0.3318  decode.d1.loss_dice: 0.5386  decode.d2.loss_cls: 0.0242  decode.d2.loss_mask: 0.3304  decode.d2.loss_dice: 0.5340  decode.d3.loss_cls: 0.0240  decode.d3.loss_mask: 0.3287  decode.d3.loss_dice: 0.5385  decode.d4.loss_cls: 0.0228  decode.d4.loss_mask: 0.3243  decode.d4.loss_dice: 0.5244  decode.d5.loss_cls: 0.0256  decode.d5.loss_mask: 0.3280  decode.d5.loss_dice: 0.5412  decode.d6.loss_cls: 0.0282  decode.d6.loss_mask: 0.3238  decode.d6.loss_dice: 0.5350  decode.d7.loss_cls: 0.0259  decode.d7.loss_mask: 0.3297  decode.d7.loss_dice: 0.5304  decode.d8.loss_cls: 0.0341  decode.d8.loss_mask: 0.3280  decode.d8.loss_dice: 0.5227
2023/06/15 21:36:03 - mmengine - INFO - Iter(train) [2280/3000]  lr: 9.8717e-06  eta: 0:05:34  time: 0.4548  data_time: 0.0044  memory: 8331  grad_norm: 177.9495  loss: 10.7036  decode.loss_cls: 0.0093  decode.loss_mask: 0.4538  decode.loss_dice: 0.5808  decode.d0.loss_cls: 0.1225  decode.d0.loss_mask: 0.4472  decode.d0.loss_dice: 0.5838  decode.d1.loss_cls: 0.0886  decode.d1.loss_mask: 0.4464  decode.d1.loss_dice: 0.5950  decode.d2.loss_cls: 0.0130  decode.d2.loss_mask: 0.4565  decode.d2.loss_dice: 0.6024  decode.d3.loss_cls: 0.0062  decode.d3.loss_mask: 0.4584  decode.d3.loss_dice: 0.5844  decode.d4.loss_cls: 0.0066  decode.d4.loss_mask: 0.4556  decode.d4.loss_dice: 0.5854  decode.d5.loss_cls: 0.0079  decode.d5.loss_mask: 0.4591  decode.d5.loss_dice: 0.5845  decode.d6.loss_cls: 0.0066  decode.d6.loss_mask: 0.4566  decode.d6.loss_dice: 0.6022  decode.d7.loss_cls: 0.0071  decode.d7.loss_mask: 0.4529  decode.d7.loss_dice: 0.5866  decode.d8.loss_cls: 0.0094  decode.d8.loss_mask: 0.4539  decode.d8.loss_dice: 0.5806
2023/06/15 21:36:16 - mmengine - INFO - Iter(train) [2310/3000]  lr: 9.8700e-06  eta: 0:05:20  time: 0.4571  data_time: 0.0040  memory: 8331  grad_norm: 291.2237  loss: 8.5598  decode.loss_cls: 0.0089  decode.loss_mask: 0.3296  decode.loss_dice: 0.4851  decode.d0.loss_cls: 0.1160  decode.d0.loss_mask: 0.3365  decode.d0.loss_dice: 0.5150  decode.d1.loss_cls: 0.0196  decode.d1.loss_mask: 0.3321  decode.d1.loss_dice: 0.5058  decode.d2.loss_cls: 0.0143  decode.d2.loss_mask: 0.3318  decode.d2.loss_dice: 0.4915  decode.d3.loss_cls: 0.0164  decode.d3.loss_mask: 0.3290  decode.d3.loss_dice: 0.5191  decode.d4.loss_cls: 0.0101  decode.d4.loss_mask: 0.3320  decode.d4.loss_dice: 0.5150  decode.d5.loss_cls: 0.0088  decode.d5.loss_mask: 0.3304  decode.d5.loss_dice: 0.5070  decode.d6.loss_cls: 0.0123  decode.d6.loss_mask: 0.3249  decode.d6.loss_dice: 0.5042  decode.d7.loss_cls: 0.0132  decode.d7.loss_mask: 0.3282  decode.d7.loss_dice: 0.4852  decode.d8.loss_cls: 0.0110  decode.d8.loss_mask: 0.3307  decode.d8.loss_dice: 0.4961
2023/06/15 21:36:30 - mmengine - INFO - Iter(train) [2340/3000]  lr: 9.8683e-06  eta: 0:05:06  time: 0.4562  data_time: 0.0041  memory: 8320  grad_norm: 125.3442  loss: 10.1929  decode.loss_cls: 0.1799  decode.loss_mask: 0.3745  decode.loss_dice: 0.5467  decode.d0.loss_cls: 0.1197  decode.d0.loss_mask: 0.3865  decode.d0.loss_dice: 0.5332  decode.d1.loss_cls: 0.0339  decode.d1.loss_mask: 0.3845  decode.d1.loss_dice: 0.5204  decode.d2.loss_cls: 0.0265  decode.d2.loss_mask: 0.3736  decode.d2.loss_dice: 0.5405  decode.d3.loss_cls: 0.0640  decode.d3.loss_mask: 0.3813  decode.d3.loss_dice: 0.5252  decode.d4.loss_cls: 0.0543  decode.d4.loss_mask: 0.3752  decode.d4.loss_dice: 0.5233  decode.d5.loss_cls: 0.0929  decode.d5.loss_mask: 0.3783  decode.d5.loss_dice: 0.5410  decode.d6.loss_cls: 0.1505  decode.d6.loss_mask: 0.3741  decode.d6.loss_dice: 0.5345  decode.d7.loss_cls: 0.1671  decode.d7.loss_mask: 0.3773  decode.d7.loss_dice: 0.5402  decode.d8.loss_cls: 0.1944  decode.d8.loss_mask: 0.3711  decode.d8.loss_dice: 0.5284
2023/06/15 21:36:44 - mmengine - INFO - Iter(train) [2370/3000]  lr: 9.8666e-06  eta: 0:04:52  time: 0.4561  data_time: 0.0043  memory: 8331  grad_norm: 242.7387  loss: 8.6133  decode.loss_cls: 0.0169  decode.loss_mask: 0.3268  decode.loss_dice: 0.4944  decode.d0.loss_cls: 0.1080  decode.d0.loss_mask: 0.3192  decode.d0.loss_dice: 0.5097  decode.d1.loss_cls: 0.0276  decode.d1.loss_mask: 0.3172  decode.d1.loss_dice: 0.5094  decode.d2.loss_cls: 0.0253  decode.d2.loss_mask: 0.3177  decode.d2.loss_dice: 0.5079  decode.d3.loss_cls: 0.0234  decode.d3.loss_mask: 0.3197  decode.d3.loss_dice: 0.5172  decode.d4.loss_cls: 0.0254  decode.d4.loss_mask: 0.3202  decode.d4.loss_dice: 0.5073  decode.d5.loss_cls: 0.0296  decode.d5.loss_mask: 0.3196  decode.d5.loss_dice: 0.4964  decode.d6.loss_cls: 0.0200  decode.d6.loss_mask: 0.3205  decode.d6.loss_dice: 0.5275  decode.d7.loss_cls: 0.0181  decode.d7.loss_mask: 0.3246  decode.d7.loss_dice: 0.5006  decode.d8.loss_cls: 0.0189  decode.d8.loss_mask: 0.3250  decode.d8.loss_dice: 0.5195
2023/06/15 21:36:57 - mmengine - INFO - Iter(train) [2400/3000]  lr: 9.8650e-06  eta: 0:04:38  time: 0.4559  data_time: 0.0042  memory: 8331  grad_norm: 155.4072  loss: 8.9141  decode.loss_cls: 0.0400  decode.loss_mask: 0.3557  decode.loss_dice: 0.4760  decode.d0.loss_cls: 0.1108  decode.d0.loss_mask: 0.3457  decode.d0.loss_dice: 0.4785  decode.d1.loss_cls: 0.0358  decode.d1.loss_mask: 0.3586  decode.d1.loss_dice: 0.4622  decode.d2.loss_cls: 0.0346  decode.d2.loss_mask: 0.3607  decode.d2.loss_dice: 0.4675  decode.d3.loss_cls: 0.0498  decode.d3.loss_mask: 0.3546  decode.d3.loss_dice: 0.4625  decode.d4.loss_cls: 0.0582  decode.d4.loss_mask: 0.3573  decode.d4.loss_dice: 0.4822  decode.d5.loss_cls: 0.0598  decode.d5.loss_mask: 0.3655  decode.d5.loss_dice: 0.4742  decode.d6.loss_cls: 0.0614  decode.d6.loss_mask: 0.3681  decode.d6.loss_dice: 0.4762  decode.d7.loss_cls: 0.0735  decode.d7.loss_mask: 0.3666  decode.d7.loss_dice: 0.4682  decode.d8.loss_cls: 0.0598  decode.d8.loss_mask: 0.3674  decode.d8.loss_dice: 0.4830
2023/06/15 21:36:57 - mmengine - INFO - Saving checkpoint at 2400 iterations
2023/06/15 21:37:04 - mmengine - INFO - per class results:
2023/06/15 21:37:04 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 98.12 | 98.77 |
|    watermelon_green   |  95.7 | 98.52 |
|    watermelon_white   | 93.91 | 96.54 |
| watermelon_seed_black | 83.94 |  91.1 |
| watermelon_seed_white |  70.5 |  84.0 |
|       Unlabeled       | 64.72 | 77.43 |
+-----------------------+-------+-------+
2023/06/15 21:37:04 - mmengine - INFO - Iter(val) [11/11]    aAcc: 97.9000  mIoU: 84.4800  mAcc: 91.0600  data_time: 0.0038  time: 0.1120
2023/06/15 21:37:17 - mmengine - INFO - Iter(train) [2430/3000]  lr: 9.8633e-06  eta: 0:04:24  time: 0.4541  data_time: 0.0042  memory: 8331  grad_norm: 325.6681  loss: 9.7990  decode.loss_cls: 0.0639  decode.loss_mask: 0.4248  decode.loss_dice: 0.4860  decode.d0.loss_cls: 0.1087  decode.d0.loss_mask: 0.4164  decode.d0.loss_dice: 0.4942  decode.d1.loss_cls: 0.0484  decode.d1.loss_mask: 0.4239  decode.d1.loss_dice: 0.4869  decode.d2.loss_cls: 0.0488  decode.d2.loss_mask: 0.4289  decode.d2.loss_dice: 0.4961  decode.d3.loss_cls: 0.0267  decode.d3.loss_mask: 0.4274  decode.d3.loss_dice: 0.4964  decode.d4.loss_cls: 0.0727  decode.d4.loss_mask: 0.4314  decode.d4.loss_dice: 0.4897  decode.d5.loss_cls: 0.0766  decode.d5.loss_mask: 0.4298  decode.d5.loss_dice: 0.4903  decode.d6.loss_cls: 0.0235  decode.d6.loss_mask: 0.4255  decode.d6.loss_dice: 0.4940  decode.d7.loss_cls: 0.0732  decode.d7.loss_mask: 0.4226  decode.d7.loss_dice: 0.4916  decode.d8.loss_cls: 0.0769  decode.d8.loss_mask: 0.4298  decode.d8.loss_dice: 0.4941
2023/06/15 21:37:31 - mmengine - INFO - Iter(train) [2460/3000]  lr: 9.8616e-06  eta: 0:04:10  time: 0.4537  data_time: 0.0044  memory: 8331  grad_norm: 166.8920  loss: 8.7854  decode.loss_cls: 0.0037  decode.loss_mask: 0.3691  decode.loss_dice: 0.4945  decode.d0.loss_cls: 0.0995  decode.d0.loss_mask: 0.3570  decode.d0.loss_dice: 0.4764  decode.d1.loss_cls: 0.0169  decode.d1.loss_mask: 0.3652  decode.d1.loss_dice: 0.5008  decode.d2.loss_cls: 0.0081  decode.d2.loss_mask: 0.3704  decode.d2.loss_dice: 0.5050  decode.d3.loss_cls: 0.0034  decode.d3.loss_mask: 0.3735  decode.d3.loss_dice: 0.4997  decode.d4.loss_cls: 0.0042  decode.d4.loss_mask: 0.3767  decode.d4.loss_dice: 0.5001  decode.d5.loss_cls: 0.0044  decode.d5.loss_mask: 0.3733  decode.d5.loss_dice: 0.4897  decode.d6.loss_cls: 0.0031  decode.d6.loss_mask: 0.3726  decode.d6.loss_dice: 0.4931  decode.d7.loss_cls: 0.0033  decode.d7.loss_mask: 0.3678  decode.d7.loss_dice: 0.4760  decode.d8.loss_cls: 0.0037  decode.d8.loss_mask: 0.3731  decode.d8.loss_dice: 0.5009
2023/06/15 21:37:44 - mmengine - INFO - Iter(train) [2490/3000]  lr: 9.8599e-06  eta: 0:03:56  time: 0.4545  data_time: 0.0043  memory: 8331  grad_norm: 171.7615  loss: 9.2771  decode.loss_cls: 0.0180  decode.loss_mask: 0.3100  decode.loss_dice: 0.5927  decode.d0.loss_cls: 0.0886  decode.d0.loss_mask: 0.3137  decode.d0.loss_dice: 0.5947  decode.d1.loss_cls: 0.0315  decode.d1.loss_mask: 0.3094  decode.d1.loss_dice: 0.5826  decode.d2.loss_cls: 0.0171  decode.d2.loss_mask: 0.3095  decode.d2.loss_dice: 0.5810  decode.d3.loss_cls: 0.0271  decode.d3.loss_mask: 0.3060  decode.d3.loss_dice: 0.5837  decode.d4.loss_cls: 0.0191  decode.d4.loss_mask: 0.3095  decode.d4.loss_dice: 0.5955  decode.d5.loss_cls: 0.0211  decode.d5.loss_mask: 0.3082  decode.d5.loss_dice: 0.5857  decode.d6.loss_cls: 0.0257  decode.d6.loss_mask: 0.3086  decode.d6.loss_dice: 0.5930  decode.d7.loss_cls: 0.0174  decode.d7.loss_mask: 0.3067  decode.d7.loss_dice: 0.5976  decode.d8.loss_cls: 0.0164  decode.d8.loss_mask: 0.3088  decode.d8.loss_dice: 0.5982
2023/06/15 21:37:58 - mmengine - INFO - Iter(train) [2520/3000]  lr: 9.8582e-06  eta: 0:03:42  time: 0.4545  data_time: 0.0042  memory: 8331  grad_norm: 224.2915  loss: 9.4242  decode.loss_cls: 0.0071  decode.loss_mask: 0.3519  decode.loss_dice: 0.5804  decode.d0.loss_cls: 0.1046  decode.d0.loss_mask: 0.3511  decode.d0.loss_dice: 0.5804  decode.d1.loss_cls: 0.0216  decode.d1.loss_mask: 0.3518  decode.d1.loss_dice: 0.5601  decode.d2.loss_cls: 0.0057  decode.d2.loss_mask: 0.3529  decode.d2.loss_dice: 0.5690  decode.d3.loss_cls: 0.0090  decode.d3.loss_mask: 0.3547  decode.d3.loss_dice: 0.5552  decode.d4.loss_cls: 0.0119  decode.d4.loss_mask: 0.3519  decode.d4.loss_dice: 0.5668  decode.d5.loss_cls: 0.0145  decode.d5.loss_mask: 0.3551  decode.d5.loss_dice: 0.5705  decode.d6.loss_cls: 0.0135  decode.d6.loss_mask: 0.3551  decode.d6.loss_dice: 0.5743  decode.d7.loss_cls: 0.0096  decode.d7.loss_mask: 0.3515  decode.d7.loss_dice: 0.5739  decode.d8.loss_cls: 0.0079  decode.d8.loss_mask: 0.3543  decode.d8.loss_dice: 0.5578
2023/06/15 21:38:12 - mmengine - INFO - Iter(train) [2550/3000]  lr: 9.8565e-06  eta: 0:03:28  time: 0.4543  data_time: 0.0040  memory: 8331  grad_norm: 136.3936  loss: 8.4349  decode.loss_cls: 0.0188  decode.loss_mask: 0.2979  decode.loss_dice: 0.5239  decode.d0.loss_cls: 0.1129  decode.d0.loss_mask: 0.2988  decode.d0.loss_dice: 0.5118  decode.d1.loss_cls: 0.0475  decode.d1.loss_mask: 0.3044  decode.d1.loss_dice: 0.5264  decode.d2.loss_cls: 0.0267  decode.d2.loss_mask: 0.3009  decode.d2.loss_dice: 0.5035  decode.d3.loss_cls: 0.0232  decode.d3.loss_mask: 0.2954  decode.d3.loss_dice: 0.5066  decode.d4.loss_cls: 0.0272  decode.d4.loss_mask: 0.2991  decode.d4.loss_dice: 0.5050  decode.d5.loss_cls: 0.0271  decode.d5.loss_mask: 0.3007  decode.d5.loss_dice: 0.5013  decode.d6.loss_cls: 0.0177  decode.d6.loss_mask: 0.3037  decode.d6.loss_dice: 0.5120  decode.d7.loss_cls: 0.0155  decode.d7.loss_mask: 0.2967  decode.d7.loss_dice: 0.5030  decode.d8.loss_cls: 0.0235  decode.d8.loss_mask: 0.2997  decode.d8.loss_dice: 0.5041
2023/06/15 21:38:25 - mmengine - INFO - Iter(train) [2580/3000]  lr: 9.8548e-06  eta: 0:03:14  time: 0.4561  data_time: 0.0044  memory: 8331  grad_norm: 185.3168  loss: 8.7677  decode.loss_cls: 0.0243  decode.loss_mask: 0.2901  decode.loss_dice: 0.5512  decode.d0.loss_cls: 0.0899  decode.d0.loss_mask: 0.2878  decode.d0.loss_dice: 0.5446  decode.d1.loss_cls: 0.0343  decode.d1.loss_mask: 0.2921  decode.d1.loss_dice: 0.5559  decode.d2.loss_cls: 0.0492  decode.d2.loss_mask: 0.2863  decode.d2.loss_dice: 0.5556  decode.d3.loss_cls: 0.0405  decode.d3.loss_mask: 0.2893  decode.d3.loss_dice: 0.5278  decode.d4.loss_cls: 0.0359  decode.d4.loss_mask: 0.2926  decode.d4.loss_dice: 0.5517  decode.d5.loss_cls: 0.0427  decode.d5.loss_mask: 0.2936  decode.d5.loss_dice: 0.5222  decode.d6.loss_cls: 0.0333  decode.d6.loss_mask: 0.2892  decode.d6.loss_dice: 0.5270  decode.d7.loss_cls: 0.0340  decode.d7.loss_mask: 0.2926  decode.d7.loss_dice: 0.5609  decode.d8.loss_cls: 0.0416  decode.d8.loss_mask: 0.2888  decode.d8.loss_dice: 0.5426
2023/06/15 21:38:39 - mmengine - INFO - Iter(train) [2610/3000]  lr: 9.8531e-06  eta: 0:03:00  time: 0.4538  data_time: 0.0042  memory: 8331  grad_norm: 121.4298  loss: 8.4853  decode.loss_cls: 0.0125  decode.loss_mask: 0.3408  decode.loss_dice: 0.5029  decode.d0.loss_cls: 0.0973  decode.d0.loss_mask: 0.3466  decode.d0.loss_dice: 0.4773  decode.d1.loss_cls: 0.0290  decode.d1.loss_mask: 0.3439  decode.d1.loss_dice: 0.4919  decode.d2.loss_cls: 0.0109  decode.d2.loss_mask: 0.3434  decode.d2.loss_dice: 0.4844  decode.d3.loss_cls: 0.0065  decode.d3.loss_mask: 0.3424  decode.d3.loss_dice: 0.4826  decode.d4.loss_cls: 0.0080  decode.d4.loss_mask: 0.3421  decode.d4.loss_dice: 0.4700  decode.d5.loss_cls: 0.0101  decode.d5.loss_mask: 0.3437  decode.d5.loss_dice: 0.4781  decode.d6.loss_cls: 0.0099  decode.d6.loss_mask: 0.3395  decode.d6.loss_dice: 0.4883  decode.d7.loss_cls: 0.0104  decode.d7.loss_mask: 0.3412  decode.d7.loss_dice: 0.4951  decode.d8.loss_cls: 0.0116  decode.d8.loss_mask: 0.3411  decode.d8.loss_dice: 0.4835
2023/06/15 21:38:53 - mmengine - INFO - Iter(train) [2640/3000]  lr: 9.8514e-06  eta: 0:02:46  time: 0.4551  data_time: 0.0040  memory: 8331  grad_norm: 199.8856  loss: 8.5224  decode.loss_cls: 0.0023  decode.loss_mask: 0.3346  decode.loss_dice: 0.5184  decode.d0.loss_cls: 0.0904  decode.d0.loss_mask: 0.3358  decode.d0.loss_dice: 0.5127  decode.d1.loss_cls: 0.0183  decode.d1.loss_mask: 0.3247  decode.d1.loss_dice: 0.5091  decode.d2.loss_cls: 0.0084  decode.d2.loss_mask: 0.3299  decode.d2.loss_dice: 0.5066  decode.d3.loss_cls: 0.0020  decode.d3.loss_mask: 0.3288  decode.d3.loss_dice: 0.5066  decode.d4.loss_cls: 0.0024  decode.d4.loss_mask: 0.3295  decode.d4.loss_dice: 0.5007  decode.d5.loss_cls: 0.0053  decode.d5.loss_mask: 0.3300  decode.d5.loss_dice: 0.5113  decode.d6.loss_cls: 0.0015  decode.d6.loss_mask: 0.3305  decode.d6.loss_dice: 0.5057  decode.d7.loss_cls: 0.0013  decode.d7.loss_mask: 0.3318  decode.d7.loss_dice: 0.4997  decode.d8.loss_cls: 0.0021  decode.d8.loss_mask: 0.3320  decode.d8.loss_dice: 0.5100
2023/06/15 21:39:06 - mmengine - INFO - Iter(train) [2670/3000]  lr: 9.8497e-06  eta: 0:02:32  time: 0.4538  data_time: 0.0041  memory: 8320  grad_norm: 189.5295  loss: 7.3190  decode.loss_cls: 0.0071  decode.loss_mask: 0.3242  decode.loss_dice: 0.4046  decode.d0.loss_cls: 0.0980  decode.d0.loss_mask: 0.3225  decode.d0.loss_dice: 0.3926  decode.d1.loss_cls: 0.0352  decode.d1.loss_mask: 0.3185  decode.d1.loss_dice: 0.3885  decode.d2.loss_cls: 0.0175  decode.d2.loss_mask: 0.3205  decode.d2.loss_dice: 0.3954  decode.d3.loss_cls: 0.0066  decode.d3.loss_mask: 0.3223  decode.d3.loss_dice: 0.3839  decode.d4.loss_cls: 0.0046  decode.d4.loss_mask: 0.3223  decode.d4.loss_dice: 0.3822  decode.d5.loss_cls: 0.0051  decode.d5.loss_mask: 0.3198  decode.d5.loss_dice: 0.3934  decode.d6.loss_cls: 0.0034  decode.d6.loss_mask: 0.3190  decode.d6.loss_dice: 0.3901  decode.d7.loss_cls: 0.0054  decode.d7.loss_mask: 0.3219  decode.d7.loss_dice: 0.3899  decode.d8.loss_cls: 0.0104  decode.d8.loss_mask: 0.3192  decode.d8.loss_dice: 0.3949
2023/06/15 21:39:20 - mmengine - INFO - Iter(train) [2700/3000]  lr: 9.8481e-06  eta: 0:02:18  time: 0.4542  data_time: 0.0041  memory: 8331  grad_norm: 223.5735  loss: 8.5337  decode.loss_cls: 0.0125  decode.loss_mask: 0.3469  decode.loss_dice: 0.4853  decode.d0.loss_cls: 0.0915  decode.d0.loss_mask: 0.3572  decode.d0.loss_dice: 0.4801  decode.d1.loss_cls: 0.0281  decode.d1.loss_mask: 0.3548  decode.d1.loss_dice: 0.4884  decode.d2.loss_cls: 0.0083  decode.d2.loss_mask: 0.3485  decode.d2.loss_dice: 0.4757  decode.d3.loss_cls: 0.0081  decode.d3.loss_mask: 0.3511  decode.d3.loss_dice: 0.4902  decode.d4.loss_cls: 0.0067  decode.d4.loss_mask: 0.3488  decode.d4.loss_dice: 0.4892  decode.d5.loss_cls: 0.0071  decode.d5.loss_mask: 0.3487  decode.d5.loss_dice: 0.4761  decode.d6.loss_cls: 0.0142  decode.d6.loss_mask: 0.3498  decode.d6.loss_dice: 0.4881  decode.d7.loss_cls: 0.0080  decode.d7.loss_mask: 0.3491  decode.d7.loss_dice: 0.4846  decode.d8.loss_cls: 0.0119  decode.d8.loss_mask: 0.3503  decode.d8.loss_dice: 0.4744
2023/06/15 21:39:20 - mmengine - INFO - Saving checkpoint at 2700 iterations
2023/06/15 21:39:26 - mmengine - INFO - per class results:
2023/06/15 21:39:26 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 98.24 | 98.96 |
|    watermelon_green   | 95.95 | 98.53 |
|    watermelon_white   | 94.62 | 96.93 |
| watermelon_seed_black | 85.51 | 91.87 |
| watermelon_seed_white | 74.41 | 85.97 |
|       Unlabeled       | 57.42 | 59.49 |
+-----------------------+-------+-------+
2023/06/15 21:39:26 - mmengine - INFO - Iter(val) [11/11]    aAcc: 98.0800  mIoU: 84.3600  mAcc: 88.6300  data_time: 0.0036  time: 0.1071
2023/06/15 21:39:40 - mmengine - INFO - Iter(train) [2730/3000]  lr: 9.8464e-06  eta: 0:02:04  time: 0.4539  data_time: 0.0040  memory: 8331  grad_norm: 353.4652  loss: 8.5395  decode.loss_cls: 0.0136  decode.loss_mask: 0.3192  decode.loss_dice: 0.5304  decode.d0.loss_cls: 0.0947  decode.d0.loss_mask: 0.3166  decode.d0.loss_dice: 0.5156  decode.d1.loss_cls: 0.0140  decode.d1.loss_mask: 0.3222  decode.d1.loss_dice: 0.4728  decode.d2.loss_cls: 0.0102  decode.d2.loss_mask: 0.3195  decode.d2.loss_dice: 0.4898  decode.d3.loss_cls: 0.0145  decode.d3.loss_mask: 0.3235  decode.d3.loss_dice: 0.5277  decode.d4.loss_cls: 0.0144  decode.d4.loss_mask: 0.3214  decode.d4.loss_dice: 0.5268  decode.d5.loss_cls: 0.0109  decode.d5.loss_mask: 0.3219  decode.d5.loss_dice: 0.5201  decode.d6.loss_cls: 0.0164  decode.d6.loss_mask: 0.3200  decode.d6.loss_dice: 0.5047  decode.d7.loss_cls: 0.0133  decode.d7.loss_mask: 0.3232  decode.d7.loss_dice: 0.5289  decode.d8.loss_cls: 0.0085  decode.d8.loss_mask: 0.3193  decode.d8.loss_dice: 0.5053
2023/06/15 21:39:53 - mmengine - INFO - Iter(train) [2760/3000]  lr: 9.8447e-06  eta: 0:01:51  time: 0.4538  data_time: 0.0039  memory: 8331  grad_norm: 177.8863  loss: 8.7983  decode.loss_cls: 0.0275  decode.loss_mask: 0.3179  decode.loss_dice: 0.5217  decode.d0.loss_cls: 0.0839  decode.d0.loss_mask: 0.3298  decode.d0.loss_dice: 0.5188  decode.d1.loss_cls: 0.0323  decode.d1.loss_mask: 0.3198  decode.d1.loss_dice: 0.5156  decode.d2.loss_cls: 0.0356  decode.d2.loss_mask: 0.3206  decode.d2.loss_dice: 0.5153  decode.d3.loss_cls: 0.0492  decode.d3.loss_mask: 0.3186  decode.d3.loss_dice: 0.5087  decode.d4.loss_cls: 0.0532  decode.d4.loss_mask: 0.3218  decode.d4.loss_dice: 0.5119  decode.d5.loss_cls: 0.0478  decode.d5.loss_mask: 0.3219  decode.d5.loss_dice: 0.5159  decode.d6.loss_cls: 0.0427  decode.d6.loss_mask: 0.3224  decode.d6.loss_dice: 0.5127  decode.d7.loss_cls: 0.0423  decode.d7.loss_mask: 0.3175  decode.d7.loss_dice: 0.4999  decode.d8.loss_cls: 0.0442  decode.d8.loss_mask: 0.3210  decode.d8.loss_dice: 0.5082
2023/06/15 21:40:07 - mmengine - INFO - Iter(train) [2790/3000]  lr: 9.8430e-06  eta: 0:01:37  time: 0.4533  data_time: 0.0041  memory: 8331  grad_norm: 67.3007  loss: 7.5620  decode.loss_cls: 0.0069  decode.loss_mask: 0.2749  decode.loss_dice: 0.4638  decode.d0.loss_cls: 0.0844  decode.d0.loss_mask: 0.2710  decode.d0.loss_dice: 0.4699  decode.d1.loss_cls: 0.0193  decode.d1.loss_mask: 0.2738  decode.d1.loss_dice: 0.4840  decode.d2.loss_cls: 0.0070  decode.d2.loss_mask: 0.2706  decode.d2.loss_dice: 0.4738  decode.d3.loss_cls: 0.0053  decode.d3.loss_mask: 0.2700  decode.d3.loss_dice: 0.4633  decode.d4.loss_cls: 0.0060  decode.d4.loss_mask: 0.2703  decode.d4.loss_dice: 0.4943  decode.d5.loss_cls: 0.0067  decode.d5.loss_mask: 0.2714  decode.d5.loss_dice: 0.4722  decode.d6.loss_cls: 0.0066  decode.d6.loss_mask: 0.2722  decode.d6.loss_dice: 0.4549  decode.d7.loss_cls: 0.0070  decode.d7.loss_mask: 0.2739  decode.d7.loss_dice: 0.4584  decode.d8.loss_cls: 0.0068  decode.d8.loss_mask: 0.2715  decode.d8.loss_dice: 0.4518
2023/06/15 21:40:21 - mmengine - INFO - Iter(train) [2820/3000]  lr: 9.8413e-06  eta: 0:01:23  time: 0.4542  data_time: 0.0041  memory: 8320  grad_norm: 463.1209  loss: 9.1823  decode.loss_cls: 0.0224  decode.loss_mask: 0.4746  decode.loss_dice: 0.5187  decode.d0.loss_cls: 0.1056  decode.d0.loss_mask: 0.4021  decode.d0.loss_dice: 0.4664  decode.d1.loss_cls: 0.0298  decode.d1.loss_mask: 0.3985  decode.d1.loss_dice: 0.4762  decode.d2.loss_cls: 0.0204  decode.d2.loss_mask: 0.4036  decode.d2.loss_dice: 0.4826  decode.d3.loss_cls: 0.0228  decode.d3.loss_mask: 0.3946  decode.d3.loss_dice: 0.4791  decode.d4.loss_cls: 0.0251  decode.d4.loss_mask: 0.3970  decode.d4.loss_dice: 0.4849  decode.d5.loss_cls: 0.0203  decode.d5.loss_mask: 0.3947  decode.d5.loss_dice: 0.4915  decode.d6.loss_cls: 0.0189  decode.d6.loss_mask: 0.3958  decode.d6.loss_dice: 0.4832  decode.d7.loss_cls: 0.0203  decode.d7.loss_mask: 0.3903  decode.d7.loss_dice: 0.4637  decode.d8.loss_cls: 0.0164  decode.d8.loss_mask: 0.4021  decode.d8.loss_dice: 0.4804
2023/06/15 21:40:34 - mmengine - INFO - Iter(train) [2850/3000]  lr: 9.8396e-06  eta: 0:01:09  time: 0.4538  data_time: 0.0042  memory: 8331  grad_norm: 202.8855  loss: 9.6845  decode.loss_cls: 0.0436  decode.loss_mask: 0.3517  decode.loss_dice: 0.6022  decode.d0.loss_cls: 0.0807  decode.d0.loss_mask: 0.3405  decode.d0.loss_dice: 0.6043  decode.d1.loss_cls: 0.0289  decode.d1.loss_mask: 0.3351  decode.d1.loss_dice: 0.5878  decode.d2.loss_cls: 0.0244  decode.d2.loss_mask: 0.3319  decode.d2.loss_dice: 0.5985  decode.d3.loss_cls: 0.0206  decode.d3.loss_mask: 0.3353  decode.d3.loss_dice: 0.5979  decode.d4.loss_cls: 0.0246  decode.d4.loss_mask: 0.3386  decode.d4.loss_dice: 0.5879  decode.d5.loss_cls: 0.0263  decode.d5.loss_mask: 0.3346  decode.d5.loss_dice: 0.5941  decode.d6.loss_cls: 0.0252  decode.d6.loss_mask: 0.3402  decode.d6.loss_dice: 0.5877  decode.d7.loss_cls: 0.0294  decode.d7.loss_mask: 0.3467  decode.d7.loss_dice: 0.5871  decode.d8.loss_cls: 0.0377  decode.d8.loss_mask: 0.3449  decode.d8.loss_dice: 0.5961
2023/06/15 21:40:48 - mmengine - INFO - Iter(train) [2880/3000]  lr: 9.8379e-06  eta: 0:00:55  time: 0.4542  data_time: 0.0040  memory: 8331  grad_norm: 99.1330  loss: 8.2131  decode.loss_cls: 0.0100  decode.loss_mask: 0.3011  decode.loss_dice: 0.4951  decode.d0.loss_cls: 0.0847  decode.d0.loss_mask: 0.3018  decode.d0.loss_dice: 0.5003  decode.d1.loss_cls: 0.0204  decode.d1.loss_mask: 0.3027  decode.d1.loss_dice: 0.5119  decode.d2.loss_cls: 0.0078  decode.d2.loss_mask: 0.3025  decode.d2.loss_dice: 0.5168  decode.d3.loss_cls: 0.0067  decode.d3.loss_mask: 0.2972  decode.d3.loss_dice: 0.4967  decode.d4.loss_cls: 0.0065  decode.d4.loss_mask: 0.3007  decode.d4.loss_dice: 0.5168  decode.d5.loss_cls: 0.0085  decode.d5.loss_mask: 0.3025  decode.d5.loss_dice: 0.5032  decode.d6.loss_cls: 0.0108  decode.d6.loss_mask: 0.2973  decode.d6.loss_dice: 0.4835  decode.d7.loss_cls: 0.0116  decode.d7.loss_mask: 0.3023  decode.d7.loss_dice: 0.5023  decode.d8.loss_cls: 0.0114  decode.d8.loss_mask: 0.2993  decode.d8.loss_dice: 0.5006
2023/06/15 21:41:02 - mmengine - INFO - Iter(train) [2910/3000]  lr: 9.8362e-06  eta: 0:00:41  time: 0.4550  data_time: 0.0042  memory: 8331  grad_norm: 153.0835  loss: 9.1675  decode.loss_cls: 0.1023  decode.loss_mask: 0.3130  decode.loss_dice: 0.4975  decode.d0.loss_cls: 0.0898  decode.d0.loss_mask: 0.3102  decode.d0.loss_dice: 0.5102  decode.d1.loss_cls: 0.0767  decode.d1.loss_mask: 0.3092  decode.d1.loss_dice: 0.4824  decode.d2.loss_cls: 0.1396  decode.d2.loss_mask: 0.3117  decode.d2.loss_dice: 0.4934  decode.d3.loss_cls: 0.1355  decode.d3.loss_mask: 0.3126  decode.d3.loss_dice: 0.5049  decode.d4.loss_cls: 0.1298  decode.d4.loss_mask: 0.3113  decode.d4.loss_dice: 0.5006  decode.d5.loss_cls: 0.1090  decode.d5.loss_mask: 0.3135  decode.d5.loss_dice: 0.5003  decode.d6.loss_cls: 0.1007  decode.d6.loss_mask: 0.3154  decode.d6.loss_dice: 0.5036  decode.d7.loss_cls: 0.1131  decode.d7.loss_mask: 0.3125  decode.d7.loss_dice: 0.4804  decode.d8.loss_cls: 0.0998  decode.d8.loss_mask: 0.3129  decode.d8.loss_dice: 0.4753
2023/06/15 21:41:15 - mmengine - INFO - Iter(train) [2940/3000]  lr: 9.8345e-06  eta: 0:00:27  time: 0.4537  data_time: 0.0040  memory: 8331  grad_norm: 210.4656  loss: 9.6482  decode.loss_cls: 0.0128  decode.loss_mask: 0.3873  decode.loss_dice: 0.5786  decode.d0.loss_cls: 0.0783  decode.d0.loss_mask: 0.3885  decode.d0.loss_dice: 0.5426  decode.d1.loss_cls: 0.0102  decode.d1.loss_mask: 0.3837  decode.d1.loss_dice: 0.5483  decode.d2.loss_cls: 0.0164  decode.d2.loss_mask: 0.3884  decode.d2.loss_dice: 0.5498  decode.d3.loss_cls: 0.0055  decode.d3.loss_mask: 0.3822  decode.d3.loss_dice: 0.5572  decode.d4.loss_cls: 0.0086  decode.d4.loss_mask: 0.3825  decode.d4.loss_dice: 0.5617  decode.d5.loss_cls: 0.0141  decode.d5.loss_mask: 0.3873  decode.d5.loss_dice: 0.5702  decode.d6.loss_cls: 0.0058  decode.d6.loss_mask: 0.3853  decode.d6.loss_dice: 0.5646  decode.d7.loss_cls: 0.0140  decode.d7.loss_mask: 0.3876  decode.d7.loss_dice: 0.5719  decode.d8.loss_cls: 0.0353  decode.d8.loss_mask: 0.3895  decode.d8.loss_dice: 0.5398
2023/06/15 21:41:29 - mmengine - INFO - Iter(train) [2970/3000]  lr: 9.8328e-06  eta: 0:00:13  time: 0.4530  data_time: 0.0040  memory: 8331  grad_norm: 106.0921  loss: 9.4507  decode.loss_cls: 0.1166  decode.loss_mask: 0.2995  decode.loss_dice: 0.5339  decode.d0.loss_cls: 0.0971  decode.d0.loss_mask: 0.2990  decode.d0.loss_dice: 0.5474  decode.d1.loss_cls: 0.1204  decode.d1.loss_mask: 0.2944  decode.d1.loss_dice: 0.5318  decode.d2.loss_cls: 0.0796  decode.d2.loss_mask: 0.2916  decode.d2.loss_dice: 0.5213  decode.d3.loss_cls: 0.0992  decode.d3.loss_mask: 0.2953  decode.d3.loss_dice: 0.5220  decode.d4.loss_cls: 0.1284  decode.d4.loss_mask: 0.2930  decode.d4.loss_dice: 0.5442  decode.d5.loss_cls: 0.1231  decode.d5.loss_mask: 0.2940  decode.d5.loss_dice: 0.5447  decode.d6.loss_cls: 0.1046  decode.d6.loss_mask: 0.2904  decode.d6.loss_dice: 0.5533  decode.d7.loss_cls: 0.1259  decode.d7.loss_mask: 0.2935  decode.d7.loss_dice: 0.5547  decode.d8.loss_cls: 0.1176  decode.d8.loss_mask: 0.2971  decode.d8.loss_dice: 0.5371
2023/06/15 21:41:43 - mmengine - INFO - Exp name: AwatermelonMask2Former_20230615_211716
2023/06/15 21:41:43 - mmengine - INFO - Iter(train) [3000/3000]  lr: 9.8311e-06  eta: 0:00:00  time: 0.4539  data_time: 0.0042  memory: 8331  grad_norm: 83.8934  loss: 8.1871  decode.loss_cls: 0.0353  decode.loss_mask: 0.2947  decode.loss_dice: 0.4915  decode.d0.loss_cls: 0.0750  decode.d0.loss_mask: 0.2962  decode.d0.loss_dice: 0.4745  decode.d1.loss_cls: 0.0250  decode.d1.loss_mask: 0.2923  decode.d1.loss_dice: 0.4826  decode.d2.loss_cls: 0.0284  decode.d2.loss_mask: 0.2937  decode.d2.loss_dice: 0.4919  decode.d3.loss_cls: 0.0367  decode.d3.loss_mask: 0.2959  decode.d3.loss_dice: 0.5054  decode.d4.loss_cls: 0.0307  decode.d4.loss_mask: 0.2922  decode.d4.loss_dice: 0.4802  decode.d5.loss_cls: 0.0379  decode.d5.loss_mask: 0.2919  decode.d5.loss_dice: 0.4946  decode.d6.loss_cls: 0.0392  decode.d6.loss_mask: 0.2947  decode.d6.loss_dice: 0.4937  decode.d7.loss_cls: 0.0405  decode.d7.loss_mask: 0.2916  decode.d7.loss_dice: 0.4778  decode.d8.loss_cls: 0.0352  decode.d8.loss_mask: 0.2939  decode.d8.loss_dice: 0.4737
2023/06/15 21:41:43 - mmengine - INFO - Saving checkpoint at 3000 iterations
2023/06/15 21:41:49 - mmengine - INFO - per class results:
2023/06/15 21:41:49 - mmengine - INFO - 
+-----------------------+-------+-------+
|         Class         |  IoU  |  Acc  |
+-----------------------+-------+-------+
|     watermelon_red    | 98.28 |  98.9 |
|    watermelon_green   | 95.92 | 98.53 |
|    watermelon_white   | 94.28 | 96.75 |
| watermelon_seed_black | 85.68 | 92.61 |
| watermelon_seed_white | 72.72 | 84.44 |
|       Unlabeled       | 56.78 | 59.59 |
+-----------------------+-------+-------+
2023/06/15 21:41:49 - mmengine - INFO - Iter(val) [11/11]    aAcc: 98.0700  mIoU: 83.9400  mAcc: 88.4700  data_time: 0.0042  time: 0.1122
